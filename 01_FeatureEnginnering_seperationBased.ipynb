{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1545df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %pip install scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SUPPLEMENT_FILE = 'supplementary_data.csv'\n",
    "INPUT_PATTERN = 'input_2023_w{:02d}.csv'\n",
    "OUTPUT_PATTERN = 'supplement-input_2023_w{:02d}.csv'\n",
    "NUM_WEEKS = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215dbc83",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42be8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ============================================================================\n",
    "\n",
    "# Pressure zone definitions (yards from QB)\n",
    "PRESSURE_ZONES = {\n",
    "    'immediate': 3.0,    # 0-3 yards: Must throw/scramble NOW\n",
    "    'closing': 5.0,      # 3-5 yards: Pressure building rapidly  \n",
    "    'potential': 7.0     # 5-7 yards: Defenders approaching\n",
    "}\n",
    "\n",
    "# Convergence classification thresholds\n",
    "CONVERGENCE_THRESHOLDS = {\n",
    "    'critical': 3,    # 3+ defenders = critical convergence\n",
    "    'high': 2,        # 2 defenders = high convergence\n",
    "    'moderate': 1,    # 1 defender = moderate convergence\n",
    "    'none': 0         # 0 defenders = no convergence\n",
    "}\n",
    "\n",
    "# Expected frame rate (frames per second)\n",
    "FRAME_RATE = 10  # NFL tracking data standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0644c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: ENHANCED COLLAPSE THRESHOLDS\n",
    "# ============================================================================\n",
    "\n",
    "ENHANCED_COLLAPSE_CONFIG = {\n",
    "    # QB Separation thresholds (yards)\n",
    "    'separation_threshold': 7.0,\n",
    "    'critical_separation': 4.0,\n",
    "    \n",
    "    # Convergence thresholds (defender count)\n",
    "    'min_converging_defenders': 2,\n",
    "    'critical_converging_defenders': 3,\n",
    "    \n",
    "    # Velocity threshold (yards/second)\n",
    "    'velocity_threshold': -5.0,\n",
    "    \n",
    "    # Zone-specific thresholds\n",
    "    'immediate_threat_zone': 3.0,\n",
    "    'closing_threat_zone': 5.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e8c56",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8658d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load supplementary data\n",
    "supplementary_data = pd.read_csv(os.path.join('data', SUPPLEMENT_FILE))\n",
    "\n",
    "# Add feature pass_category based on pass_length. \n",
    "# pass_categories are 'short', 'intermediate', and 'long'. Short is <=5 yards, intermediate is >5 and <=15 yards, long is >15 yards. Use dict mapping.\n",
    "conditions = [\n",
    "    supplementary_data['pass_length'] < 5,\n",
    "    (supplementary_data['pass_length'] >= 5) & (supplementary_data['pass_length'] <= 15),\n",
    "    supplementary_data['pass_length'] > 15\n",
    "]\n",
    "\n",
    "categories = ['short', 'intermediate', 'long']\n",
    "\n",
    "supplementary_data['pass_type'] = np.select(\n",
    "    conditions, \n",
    "    categories, \n",
    "    default='unknown'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c778266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18009, 42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supplementary_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264137c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 weeks of data with 4880579 total rows\n",
      "Test : 18\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all weekly data into one datase\n",
    "# Load all weeks data into one dataframe\n",
    "input_data_frames = []\n",
    "for week in range(1, NUM_WEEKS + 1):\n",
    "    file_path = os.path.join('data', 'input', INPUT_PATTERN.format(week))\n",
    "    df_week = pd.read_csv(file_path)\n",
    "    df_week['week'] = week  # Add week column to track the source\n",
    "    input_data_frames.append(df_week)\n",
    "\n",
    "# Combine all dataframes\n",
    "input_data = pd.concat(input_data_frames, ignore_index=True)\n",
    "print(f\"Loaded {len(input_data_frames)} weeks of data with {len(input_data)} total rows\")\n",
    "print(f\"Test : {input_data['week'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db83dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the supplementary column 'route_of_targeted_receiver' into the input data\n",
    "input_data = pd.merge(\n",
    "    input_data,                 # The left DataFrame (the one receiving the new column)\n",
    "    supplementary_data[['game_id', 'play_id', 'route_of_targeted_receiver']],  # The right DataFrame (only the key columns and the column to add)\n",
    "    on=['game_id', 'play_id'],  # The key columns to match on\n",
    "    how='left'           # Specifies a left join\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea301bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4880579, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616e0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique plays in input data: 14108\n",
      "Total unique plays after grouping: 14108\n"
     ]
    }
   ],
   "source": [
    "# list total unique plays in the input data\n",
    "unique_plays = input_data[['game_id', 'play_id']].drop_duplicates()\n",
    "print(f\"Total unique plays in input data: {len(unique_plays)}\")\n",
    "\n",
    "# group by game_id and play_id and count\n",
    "play_counts = unique_plays.groupby(['game_id', 'play_id']).size().reset_index(name='count')\n",
    "print(f\"Total unique plays after grouping: {len(play_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbda20",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "323b73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euclidean_distance_vectorized(tr_x, tr_y, def_x, def_y):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between Targeted Receiver and all defenders using vectorized operations.\n",
    "    \n",
    "    Parameters:\n",
    "    tr_x, tr_y: Quaterback coordinates (scalar)\n",
    "    def_x, def_y: Defender coordinates (arrays/series)\n",
    "    \n",
    "    Returns:\n",
    "    Array: Euclidean distances to all defenders\n",
    "    \"\"\"\n",
    "    return np.sqrt((def_x - tr_x)**2 + (def_y - tr_y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83667feb",
   "metadata": {},
   "source": [
    "### Frame Level - QB \n",
    "#### QB separation from nearest defender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43df7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frame_level_separation_qb(df):\n",
    "    \"\"\"\n",
    "    Calculate minimum separation between Quarterback and defensive players at FRAME level.\n",
    "    Returns only Quarterback data with qb_min_separation per frame.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame containing tracking data with columns:\n",
    "        - game_id, play_id, frame_id, nfl_id\n",
    "        - player_role: 'Passer' or 'Defensive Coverage'\n",
    "        - player_name\n",
    "        - x, y: Player coordinates\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Quarterback data with columns (game_id, play_id, nfl_id, frame_id, player_name, player_role, qb_min_separation)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by game, play, and frame (FRAME-LEVEL calculation)\n",
    "    grouped = df.groupby(['game_id', 'play_id', 'frame_id'])\n",
    "    \n",
    "    # List to store Quarterback results\n",
    "    qb_results = []\n",
    "    \n",
    "    for (game_id, play_id, frame_id), frame_group in grouped:\n",
    "        # Identify Quarterbacks and defenders in this specific frame\n",
    "        quarterbacks = frame_group[frame_group['player_role'] == 'Passer']\n",
    "        defenders = frame_group[frame_group['player_role'] == 'Defensive Coverage']\n",
    "        \n",
    "        # ✅ HANDLE MISSING DATA INSTEAD OF SKIPPING\n",
    "        if quarterbacks.empty:\n",
    "            continue  # No QB = can't analyze this frame\n",
    "            \n",
    "        if defenders.empty:\n",
    "            # No defenders = maximum separation (safe pocket)\n",
    "            for qb_idx, qb_row in quarterbacks.iterrows():\n",
    "                qb_result = {\n",
    "                    'game_id': game_id,\n",
    "                    'play_id': play_id,\n",
    "                    'nfl_id': qb_row['nfl_id'],\n",
    "                    'frame_id': frame_id,\n",
    "                    'player_name': qb_row['player_name'],\n",
    "                    'player_role': qb_row['player_role'],\n",
    "                    'player_position': qb_row['player_position'],\n",
    "                    'qb_min_separation': 999.0  # Very large value = no pressure\n",
    "                }\n",
    "                qb_results.append(qb_result)\n",
    "            continue\n",
    "        \n",
    "        # Extract defender coordinates as arrays for vectorized calculation\n",
    "        def_x = defenders['x'].values\n",
    "        def_y = defenders['y'].values\n",
    "        \n",
    "        # For Quarterback, calculate distance to all defenders using vectorization\n",
    "        for qb_idx, qb_row in quarterbacks.iterrows():\n",
    "            qb_x, qb_y = qb_row['x'], qb_row['y']\n",
    "            \n",
    "            # Vectorized distance calculation across all defenders in this frame\n",
    "            distances = calculate_euclidean_distance_vectorized(qb_x, qb_y, def_x, def_y)\n",
    "            \n",
    "            # Find minimum distance for this frame\n",
    "            min_distance = np.round(np.min(distances), 2)\n",
    "            \n",
    "            # Create result record for this Quarterback\n",
    "            qb_result = {\n",
    "                'game_id': game_id,\n",
    "                'play_id': play_id,\n",
    "                'nfl_id': qb_row['nfl_id'],\n",
    "                'frame_id': frame_id,\n",
    "                'player_name': qb_row['player_name'],\n",
    "                'player_role': qb_row['player_role'],\n",
    "                'player_position': qb_row['player_position'],\n",
    "                'qb_min_separation': min_distance\n",
    "            }\n",
    "            qb_results.append(qb_result)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(qb_results)\n",
    "    \n",
    "    # Sort by game, play, quarterback, and frame for chronological analysis\n",
    "    if not result_df.empty:\n",
    "        result_df = result_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8524b2b",
   "metadata": {},
   "source": [
    "#### QB Presssure Velocity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05cbdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 4.1 QB PRESSURE VELOCITY CALCULATION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def calculate_frame_level_pressure_velocity_qb(qb_separation_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate rate of change in QB separation (pressure velocity) and acceleration.\n",
    "    \n",
    "    Pressure velocity indicates how fast defenders are closing in on the quarterback.\n",
    "    Negative velocity means increasing pressure (defenders getting closer).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    qb_separation_df : DataFrame\n",
    "        Frame-level QB separation data with columns:\n",
    "        ['game_id', 'play_id', 'nfl_id', 'frame_id', 'qb_min_separation']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with additional columns:\n",
    "        - Change in separation from previous frame (yards)\n",
    "        - separation_velocity: Rate of change (yards/second)\n",
    "        - pressure_acceleration: Rate of change of velocity (yards/second²)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    - Negative velocity: Defenders closing in (pressure increasing)\n",
    "    - Positive velocity: QB creating space (pressure decreasing)\n",
    "    - Negative acceleration = Defender speeding up toward QB (pressure intensifying)\n",
    "    - Positive acceleration = Defender slowing down (pressure decreasing)\n",
    "    - First frame of each play will have NaN velocity (no previous frame)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> qb_velocity_frame_level = calculate_qb_pressure_velocity(qb_separation_df)\n",
    "    >>> qb_velocity_frame_level[['frame_id', 'qb_min_separation', 'separation_velocity']].head()\n",
    "       frame_id  qb_min_separation  separation_velocity\n",
    "    0         1               10.5                  NaN\n",
    "    1         2               10.2                 -3.0\n",
    "    2         3                9.8                 -4.0\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    required_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'qb_min_separation']\n",
    "    missing_cols = set(required_cols) - set(qb_separation_df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original data\n",
    "    df = qb_separation_df.sort_values(\n",
    "        ['game_id', 'play_id', 'nfl_id', 'frame_id']\n",
    "    ).copy()\n",
    "    \n",
    "    # Group by play to ensure calculations don't cross play boundaries\n",
    "    grouping_cols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    # Calculate frame-to-frame separation change\n",
    "    df['separation_diff'] = df.groupby(grouping_cols)['qb_min_separation'].diff()\n",
    "    \n",
    "    # Convert to velocity (yards per second)\n",
    "    # Frame interval = 0.1 seconds\n",
    "    FRAME_INTERVAL = 0.1\n",
    "    df['separation_velocity'] = (df['separation_diff'] / FRAME_INTERVAL).round(2)\n",
    "    \n",
    "    # Calculate acceleration (change in velocity)\n",
    "    df['velocity_diff'] = df.groupby(grouping_cols)['separation_velocity'].diff()\n",
    "    df['pressure_acceleration'] = (df['velocity_diff'] / FRAME_INTERVAL).round(2)\n",
    "    \n",
    "    # Clean up intermediate columns\n",
    "    df = df.drop(columns=['separation_diff', 'velocity_diff'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c69c6d",
   "metadata": {},
   "source": [
    "#### Defender Convergence Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5168c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CALCULATE DEFENDER CONVERGENCE Frame Level\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_frame_level_defender_convergence(\n",
    "    input_data: pd.DataFrame,\n",
    "    zones: Optional[Dict[str, float]] = None,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate defender convergence metrics for each frame.\n",
    "    \n",
    "    For every frame in the dataset, counts how many defenders are within\n",
    "    defined pressure zones around the quarterback. Returns frame-level\n",
    "    metrics suitable for ML feature engineering.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : pd.DataFrame\n",
    "        Frame-level tracking data containing:\n",
    "        \n",
    "        Required columns:\n",
    "        - game_id : int\n",
    "            Unique game identifier\n",
    "        - play_id : int\n",
    "            Unique play identifier within game\n",
    "        - frame_id : int\n",
    "            Frame number (10 frames per second)\n",
    "        - nfl_id : int\n",
    "            Player NFL ID\n",
    "        - player_role : str\n",
    "            'Passer' for QB, 'Defensive Coverage' for defenders\n",
    "        - x : float\n",
    "            Player x-coordinate on field (yards)\n",
    "        - y : float\n",
    "            Player y-coordinate on field (yards)\n",
    "        \n",
    "        Optional columns:\n",
    "        - player_name : str\n",
    "            Player name (for readability)\n",
    "        - player_position : str\n",
    "            Position code (QB, DE, LB, etc.)\n",
    "    \n",
    "    zones : Dict[str, float], optional\n",
    "        Custom pressure zone definitions (yards from QB).\n",
    "        Default uses PRESSURE_ZONES constant:\n",
    "        {'immediate': 3.0, 'closing': 5.0, 'potential': 7.0}\n",
    "    \n",
    "    verbose : bool, default=True\n",
    "        Print progress messages during processing.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Frame-level convergence metrics with columns:\n",
    "        \n",
    "        Identifiers:\n",
    "        - game_id, play_id, frame_id, nfl_id : int\n",
    "            Frame and player identifiers\n",
    "        - player_name, player_role : str\n",
    "            QB identification\n",
    "        \n",
    "        Convergence Counts:\n",
    "        - defenders_immediate_zone : int\n",
    "            Defenders in immediate threat zone (0-3 yards)\n",
    "        - defenders_closing_zone : int\n",
    "            Defenders in closing threat zone (0-5 yards)\n",
    "        - defenders_potential_zone : int\n",
    "            Defenders in potential threat zone (0-7 yards)\n",
    "        - total_converging_defenders : int\n",
    "            Total defenders within outermost zone\n",
    "        \n",
    "        Convergence Classification:\n",
    "        - convergence_category : str\n",
    "            'Critical', 'High', 'Moderate', or 'None'\n",
    "        \n",
    "        Validation Metrics:\n",
    "        - closest_defender_distance : float\n",
    "            Distance to nearest defender (yards)\n",
    "        - total_defenders_on_field : int\n",
    "            Total defenders in frame (for validation)\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing from input_data.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    **Pressure Zone Definitions:**\n",
    "    \n",
    "    Immediate Threat (0-3 yards):\n",
    "    - QB must make decision NOW\n",
    "    - Throw, scramble, or take sack imminent\n",
    "    \n",
    "    Closing Threat (3-5 yards):\n",
    "    - Pressure building rapidly\n",
    "    - QB has 0.5-1.0 seconds before immediate threat\n",
    "    \n",
    "    Potential Threat (5-7 yards):\n",
    "    - Defenders approaching\n",
    "    - QB has 1.0-1.5 seconds if pocket holds\n",
    "    \n",
    "    **Convergence Categories:**\n",
    "    \n",
    "    Critical (3+ defenders in 7-yard zone):\n",
    "    - Pocket collapsing from multiple angles\n",
    "    - Likely sack or forced quick throw\n",
    "    \n",
    "    High (2 defenders in 7-yard zone):\n",
    "    - Significant pressure\n",
    "    - Limited throwing lanes\n",
    "    \n",
    "    Moderate (1 defender in 7-yard zone):\n",
    "    - Manageable pressure\n",
    "    - Single rush threat\n",
    "    \n",
    "    None (0 defenders in 7-yard zone):\n",
    "    - Clean pocket\n",
    "    - QB has time to read defense\n",
    "    \n",
    "    **Performance:**\n",
    "    Processing speed: ~1000-2000 frames per second\n",
    "    Memory usage: ~50MB per 10,000 frames\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage with default zones\n",
    "    >>> convergence_df = calculate_frame_level_defender_convergence(input_data)\n",
    "    >>> \n",
    "    >>> # Custom zones for different analysis\n",
    "    >>> custom_zones = {'immediate': 2.5, 'closing': 4.0, 'potential': 6.0}\n",
    "    >>> convergence_df = calculate_frame_level_defender_convergence(\n",
    "    ...     input_data, \n",
    "    ...     zones=custom_zones,\n",
    "    ...     verbose=False\n",
    "    ... )\n",
    "    >>> \n",
    "    >>> # Analyze high convergence frames\n",
    "    >>> high_convergence = convergence_df[\n",
    "    ...     convergence_df['convergence_category'].isin(['Critical', 'High'])\n",
    "    ... ]\n",
    "    >>> print(f\"High convergence frames: {len(high_convergence)}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # INPUT VALIDATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    required_columns = [\n",
    "        'game_id', 'play_id', 'frame_id', 'nfl_id',\n",
    "        'player_role', 'x', 'y'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns \n",
    "                       if col not in input_data.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns for convergence calculation: \"\n",
    "            f\"{missing_columns}\\n\"\n",
    "            f\"Required: {required_columns}\"\n",
    "        )\n",
    "    \n",
    "    # Use provided zones or defaults\n",
    "    if zones is None:\n",
    "        zones = PRESSURE_ZONES.copy()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESSING INITIALIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"CALCULATING DEFENDER CONVERGENCE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Pressure Zones: {zones}\")\n",
    "        print(\"-\"*70)\n",
    "        print(\"Processing frames...\")\n",
    "    \n",
    "    # Group data by frame for efficient processing\n",
    "    grouped = input_data.groupby(['game_id', 'play_id', 'frame_id'])\n",
    "    total_frames = len(grouped)\n",
    "    \n",
    "    # Storage for convergence results\n",
    "    convergence_results = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    frames_processed = 0\n",
    "    frames_with_data = 0\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # FRAME-LEVEL PROCESSING LOOP\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    for (game_id, play_id, frame_id), frame_group in grouped:\n",
    "        \n",
    "        frames_processed += 1\n",
    "        \n",
    "        # Progress reporting (every 1000 frames)\n",
    "        if verbose and frames_processed % 1000 == 0:\n",
    "            print(f\"  Processed {frames_processed:,} / {total_frames:,} frames...\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # IDENTIFY QB AND DEFENDERS IN FRAME\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        qb_data = frame_group[frame_group['player_role'] == 'Passer']\n",
    "        defender_data = frame_group[\n",
    "            frame_group['player_role'] == 'Defensive Coverage'\n",
    "        ]\n",
    "        \n",
    "        # Skip frames without QB or defenders\n",
    "        if qb_data.empty or defender_data.empty:\n",
    "            continue\n",
    "        \n",
    "        frames_with_data += 1\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # PROCESS EACH QB IN FRAME (typically only 1)\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        for qb_idx, qb_row in qb_data.iterrows():\n",
    "            \n",
    "            # Extract QB information\n",
    "            qb_x = qb_row['x']\n",
    "            qb_y = qb_row['y']\n",
    "            qb_nfl_id = qb_row['nfl_id']\n",
    "            qb_name = qb_row.get('player_name', 'Unknown QB')\n",
    "            qb_position = qb_row.get('player_position', 'QB')\n",
    "            \n",
    "            # Extract defender positions as arrays (for vectorization)\n",
    "            defender_x = defender_data['x'].values\n",
    "            defender_y = defender_data['y'].values\n",
    "            total_defenders = len(defender_x)\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # CALCULATE DISTANCES (VECTORIZED)\n",
    "            # -----------------------------------------------------------------\n",
    "            \n",
    "            distances = calculate_euclidean_distance_vectorized(\n",
    "                qb_x, qb_y, defender_x, defender_y\n",
    "            )\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # COUNT DEFENDERS IN EACH ZONE\n",
    "            # -----------------------------------------------------------------\n",
    "            \n",
    "            # Immediate threat zone (0-3 yards)\n",
    "            defenders_immediate_zone = np.sum(\n",
    "                distances < zones['immediate']\n",
    "            )\n",
    "            \n",
    "            # Closing threat zone (3-5 yards)\n",
    "            defenders_closing_zone = np.sum(\n",
    "                (distances >= zones['immediate']) & (distances < zones['closing'])\n",
    "            )\n",
    "            \n",
    "            # Potential threat zone (5-7 yards)\n",
    "            defenders_potential_zone = np.sum(\n",
    "                (distances >= zones['closing']) & (distances < zones['potential'])\n",
    "            )\n",
    "            \n",
    "            # Total converging defenders (outermost zone)\n",
    "            total_converging = defenders_potential_zone + defenders_closing_zone + defenders_immediate_zone\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # CLASSIFY CONVERGENCE LEVEL\n",
    "            # -----------------------------------------------------------------\n",
    "            \n",
    "            if total_converging >= CONVERGENCE_THRESHOLDS['critical']:\n",
    "                convergence_category = 'Critical'\n",
    "            elif total_converging >= CONVERGENCE_THRESHOLDS['high']:\n",
    "                convergence_category = 'High'\n",
    "            elif total_converging >= CONVERGENCE_THRESHOLDS['moderate']:\n",
    "                convergence_category = 'Moderate'\n",
    "            else:\n",
    "                convergence_category = 'None'\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # VALIDATION METRICS\n",
    "            # -----------------------------------------------------------------\n",
    "            \n",
    "            # Closest defender distance (should match qb_min_separation)\n",
    "            closest_defender_distance = float(np.min(distances))\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # STORE FRAME RESULT\n",
    "            # -----------------------------------------------------------------\n",
    "            \n",
    "            frame_result = {\n",
    "                # Identifiers\n",
    "                'game_id': game_id,\n",
    "                'play_id': play_id,\n",
    "                'frame_id': frame_id,\n",
    "                'nfl_id': qb_nfl_id,\n",
    "                \n",
    "                # QB information\n",
    "                'player_name': qb_name,\n",
    "                'player_role': qb_row['player_role'],\n",
    "                'player_position': qb_position,\n",
    "                \n",
    "                # Convergence counts (zone-based)\n",
    "                'defenders_immediate_zone': int(defenders_immediate_zone),\n",
    "                'defenders_closing_zone': int(defenders_closing_zone),\n",
    "                'defenders_potential_zone': int(defenders_potential_zone),\n",
    "                'total_converging_defenders': int(total_converging),\n",
    "                \n",
    "                # Convergence classification\n",
    "                'convergence_category': convergence_category,\n",
    "                \n",
    "                # Validation metrics\n",
    "                'closest_defender_distance': closest_defender_distance,\n",
    "                'total_defenders_on_field': total_defenders\n",
    "            }\n",
    "            \n",
    "            convergence_results.append(frame_result)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CREATE OUTPUT DATAFRAME\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    convergence_df = pd.DataFrame(convergence_results)\n",
    "    \n",
    "    # Sort by game, play, QB, and frame for chronological analysis\n",
    "    if not convergence_df.empty:\n",
    "        convergence_df = convergence_df.sort_values(\n",
    "            ['game_id', 'play_id', 'nfl_id', 'frame_id']\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PROCESSING SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"CONVERGENCE CALCULATION COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Total frames processed: {frames_processed:,}\")\n",
    "        print(f\"Frames with QB & defenders: {frames_with_data:,}\")\n",
    "        print(f\"Convergence records created: {len(convergence_df):,}\")\n",
    "        \n",
    "        if not convergence_df.empty:\n",
    "            print(f\"\\nConvergence Category Distribution:\")\n",
    "            category_counts = convergence_df['convergence_category'].value_counts()\n",
    "            for category in ['Critical', 'High', 'Moderate', 'None']:\n",
    "                count = category_counts.get(category, 0)\n",
    "                pct = (count / len(convergence_df) * 100) if len(convergence_df) > 0 else 0\n",
    "                print(f\"  • {category:12s}: {count:5,} frames ({pct:5.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nDefender Zone Statistics:\")\n",
    "            print(f\"  • Avg defenders within 3yds: \"\n",
    "                  f\"{convergence_df['defenders_immediate_zone'].mean():.2f}\")\n",
    "            print(f\"  • Avg defenders within 5yds: \"\n",
    "                  f\"{convergence_df['defenders_closing_zone'].mean():.2f}\")\n",
    "            print(f\"  • Avg defenders within 7yds: \"\n",
    "                  f\"{convergence_df['defenders_potential_zone'].mean():.2f}\")\n",
    "            \n",
    "            print(f\"\\nClosest Defender Statistics:\")\n",
    "            print(f\"  • Min distance: \"\n",
    "                  f\"{convergence_df['closest_defender_distance'].min():.2f} yards\")\n",
    "            print(f\"  • Avg distance: \"\n",
    "                  f\"{convergence_df['closest_defender_distance'].mean():.2f} yards\")\n",
    "            print(f\"  • Max distance: \"\n",
    "                  f\"{convergence_df['closest_defender_distance'].max():.2f} yards\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return convergence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21145bc",
   "metadata": {},
   "source": [
    "#### Pocket collapse detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6335f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_enhanced_collapse_indicator(\n",
    "#     merged_df: pd.DataFrame,\n",
    "#     config: Optional[Dict] = None,\n",
    "#     verbose: bool = True\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Create enhanced pocket collapse indicator using multi-dimensional criteria.\n",
    "    \n",
    "#     Applies sophisticated collapse detection logic that combines QB separation,\n",
    "#     defender convergence, and velocity metrics. Produces both binary collapse\n",
    "#     flags and severity classifications for each frame.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     merged_df : pd.DataFrame\n",
    "#         Frame-level data with both QB separation and convergence metrics.\n",
    "        \n",
    "#         Required columns:\n",
    "#         - qb_min_separation : float\n",
    "#         - defenders_immediate_zone : int\n",
    "#         - defenders_closing_zone : int\n",
    "#         - defenders_potential_zone : int\n",
    "        \n",
    "#         Optional columns:\n",
    "#         - separation_velocity : float\n",
    "    \n",
    "#     config : Dict, optional\n",
    "#         Custom configuration for collapse thresholds.\n",
    "#         If None, uses ENHANCED_COLLAPSE_CONFIG constant.\n",
    "    \n",
    "#     verbose : bool, default=True\n",
    "#         Print collapse detection statistics and diagnostics.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Input dataframe with additional collapse-related columns:\n",
    "#         - collapse_factor_separation : bool\n",
    "#         - collapse_factor_convergence : bool\n",
    "#         - collapse_factor_immediate_threat : bool\n",
    "#         - collapse_factor_velocity : bool\n",
    "#         - is_pocket_collapse : bool\n",
    "#         - collapse_severity : str\n",
    "    \n",
    "#     Raises\n",
    "#     ------\n",
    "#     ValueError\n",
    "#         If required columns are missing from input DataFrame.\n",
    "#     TypeError\n",
    "#         If merged_df is not a pandas DataFrame.\n",
    "    \n",
    "#     Examples\n",
    "#     --------\n",
    "#     >>> enhanced_df = create_enhanced_collapse_indicator(merged_features)\n",
    "#     >>> collapse_rate = enhanced_df['is_pocket_collapse'].mean()\n",
    "#     >>> print(f\"Collapse rate: {collapse_rate:.1%}\")\n",
    "    \n",
    "#     Notes\n",
    "#     -----\n",
    "#     Enhanced Collapse Logic detects pocket collapse if ANY of these conditions are true:\n",
    "#     1. Separation + Convergence: QB separation < 7 yds AND 2+ defenders within 7-yard zone\n",
    "#     2. High Convergence (swarm): 3+ defenders within 5-yard zone\n",
    "#     3. Immediate Threat: 2+ defenders within 3-yard immediate zone\n",
    "#     4. Velocity + Convergence: Rapid closure (< -5 yds/sec) AND 1+ defenders within 5-yard zone\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # INPUT VALIDATION\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     # Check if input is None\n",
    "#     if merged_df is None:\n",
    "#         raise ValueError(\n",
    "#             \"Input DataFrame is None. \"\n",
    "#             \"Please ensure merged_pressure_features is created successfully.\"\n",
    "#         )\n",
    "    \n",
    "#     # Check if input is a DataFrame\n",
    "#     if not isinstance(merged_df, pd.DataFrame):\n",
    "#         raise TypeError(\n",
    "#             f\"Expected pandas DataFrame, got {type(merged_df).__name__}. \"\n",
    "#             f\"Please provide a valid merged DataFrame.\"\n",
    "#         )\n",
    "    \n",
    "#     # Check if DataFrame is empty\n",
    "#     if merged_df.empty:\n",
    "#         raise ValueError(\n",
    "#             \"Input DataFrame is empty. \"\n",
    "#             \"Please ensure data is loaded and merged correctly.\"\n",
    "#         )\n",
    "    \n",
    "#     # Define required columns\n",
    "#     required_columns = [\n",
    "#         'qb_min_separation',\n",
    "#         'defenders_immediate_zone',\n",
    "#         'defenders_closing_zone',\n",
    "#         'defenders_potential_zone'\n",
    "#     ]\n",
    "    \n",
    "#     # Check for missing columns\n",
    "#     missing_columns = [col for col in required_columns if col not in merged_df.columns]\n",
    "    \n",
    "#     if missing_columns:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"ERROR: MISSING REQUIRED COLUMNS\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"Missing columns: {missing_columns}\")\n",
    "#         print(f\"\\nAvailable columns in input DataFrame:\")\n",
    "#         print(merged_df.columns.tolist())\n",
    "#         print(\"=\"*70)\n",
    "#         raise ValueError(\n",
    "#             f\"Missing required columns: {missing_columns}\\n\"\n",
    "#             f\"Required columns: {required_columns}\\n\"\n",
    "#             f\"Please ensure you've merged QB separation with convergence metrics.\"\n",
    "#         )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONFIGURATION SETUP\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     # Use provided config or default\n",
    "#     if config is None:\n",
    "#         config = ENHANCED_COLLAPSE_CONFIG\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"CREATING ENHANCED POCKET COLLAPSE INDICATOR\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"Input DataFrame shape: {merged_df.shape}\")\n",
    "#         print(f\"Method: Multi-dimensional collapse detection\")\n",
    "#         print(f\"\\nConfiguration:\")\n",
    "#         for key, value in config.items():\n",
    "#             print(f\"  • {key}: {value}\")\n",
    "#         print(\"-\"*70)\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # DATA PREPARATION\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     try:\n",
    "#         # Create working copy to avoid modifying original data\n",
    "#         df = merged_df.copy()\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\nDataFrame copied successfully\")\n",
    "#             print(f\"Working with {len(df):,} frames\")\n",
    "#             print(f\"\\nAvailable columns: {df.columns.tolist()}\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"ERROR: Failed to copy DataFrame\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"Error type: {type(e).__name__}\")\n",
    "#         print(f\"Error message: {str(e)}\")\n",
    "#         print(\"=\"*70)\n",
    "#         raise\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # EXTRACT CONFIGURATION PARAMETERS\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     sep_threshold = config.get('separation_threshold', 7.0)\n",
    "#     critical_sep = config.get('critical_separation', 4.0)\n",
    "#     min_converging = config.get('min_converging_defenders', 2)\n",
    "#     critical_converging = config.get('critical_converging_defenders', 3)\n",
    "#     velocity_threshold = config.get('velocity_threshold', -5.0)\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(f\"\\nConfiguration extracted:\")\n",
    "#         print(f\"  • Separation threshold: {sep_threshold} yards\")\n",
    "#         print(f\"  • Critical separation: {critical_sep} yards\")\n",
    "#         print(f\"  • Min converging defenders: {min_converging}\")\n",
    "#         print(f\"  • Critical converging defenders: {critical_converging}\")\n",
    "#         print(f\"  • Velocity threshold: {velocity_threshold} yds/sec\")\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 1: SEPARATION + CONVERGENCE COLLAPSE\n",
    "#     # =========================================================================\n",
    "#     # QB separation low AND multiple defenders converging\n",
    "    \n",
    "#     df['collapse_factor_separation'] = (\n",
    "#         (df['qb_min_separation'] < sep_threshold) &\n",
    "#         (df['defenders_potential_zone'] >= min_converging)\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 2: HIGH CONVERGENCE (SWARM)\n",
    "#     # =========================================================================\n",
    "#     # 3+ defenders within 5-yard zone regardless of QB separation\n",
    "    \n",
    "#     df['collapse_factor_convergence'] = (\n",
    "#         df['defenders_closing_zone'] >= critical_converging\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 3: IMMEDIATE THREAT\n",
    "#     # =========================================================================\n",
    "#     # 2+ defenders within 3-yard immediate zone (imminent sack)\n",
    "    \n",
    "#     df['collapse_factor_immediate_threat'] = (\n",
    "#         df['defenders_immediate_zone'] >= 2\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 4: VELOCITY + CONVERGENCE (IF VELOCITY AVAILABLE)\n",
    "#     # =========================================================================\n",
    "#     # Rapid closure (velocity < -5 yds/sec) AND defenders within 5-yard zone\n",
    "    \n",
    "#     if 'separation_velocity' in df.columns:\n",
    "#         # Check for non-null velocity values\n",
    "#         has_velocity_data = df['separation_velocity'].notna().any()\n",
    "        \n",
    "#         if has_velocity_data:\n",
    "#             df['collapse_factor_velocity'] = (\n",
    "#                 (df['separation_velocity'] < velocity_threshold) &\n",
    "#                 (df['defenders_closing_zone'] >= 1)\n",
    "#             )\n",
    "#             if verbose:\n",
    "#                 print(f\"\\nVelocity-based detection enabled\")\n",
    "#                 print(f\"  • Frames with velocity data: {df['separation_velocity'].notna().sum():,}\")\n",
    "#         else:\n",
    "#             df['collapse_factor_velocity'] = False\n",
    "#             if verbose:\n",
    "#                 print(f\"\\nVelocity column exists but contains no valid data\")\n",
    "#     else:\n",
    "#         df['collapse_factor_velocity'] = False\n",
    "#         if verbose:\n",
    "#             print(f\"\\nVelocity-based detection disabled (no velocity column)\")\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # PRIMARY COLLAPSE INDICATOR (ANY CONDITION TRIGGERS COLLAPSE)\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     df['is_pocket_collapse'] = (\n",
    "#         df['collapse_factor_separation'] |\n",
    "#         df['collapse_factor_convergence'] |\n",
    "#         df['collapse_factor_immediate_threat'] |\n",
    "#         df['collapse_factor_velocity']\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # COLLAPSE SEVERITY CLASSIFICATION\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     # Define severity conditions (evaluated in order of severity)\n",
    "#     conditions = [\n",
    "#         # Critical: 2+ defenders within 3 yards OR separation < 4 with 3+ converging\n",
    "#         (df['defenders_immediate_zone'] >= 2) | \n",
    "#         ((df['qb_min_separation'] < critical_sep) & \n",
    "#          (df['defenders_potential_zone'] >= critical_converging)),\n",
    "        \n",
    "#         # Severe: 3+ defenders within 5 yards OR separation < 5 with 2+ converging\n",
    "#         (df['defenders_closing_zone'] >= critical_converging) |\n",
    "#         ((df['qb_min_separation'] < 5.0) & \n",
    "#          (df['defenders_potential_zone'] >= min_converging)),\n",
    "        \n",
    "#         # Moderate: 2+ defenders within 7 yards AND separation < 7\n",
    "#         (df['defenders_potential_zone'] >= min_converging) & \n",
    "#         (df['qb_min_separation'] < sep_threshold),\n",
    "        \n",
    "#         # Light: Separation < 7 BUT only 0-1 converging defenders\n",
    "#         (df['qb_min_separation'] < sep_threshold) & \n",
    "#         (df['defenders_potential_zone'] < min_converging)\n",
    "#     ]\n",
    "    \n",
    "#     severity_levels = ['Critical', 'Severe', 'Moderate', 'Light']\n",
    "#     df['collapse_severity'] = np.select(conditions, severity_levels, default='None')\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # SUMMARY STATISTICS\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"COLLAPSE DETECTION COMPLETE\")\n",
    "#         print(\"=\"*70)\n",
    "        \n",
    "#         print(f\"\\nOverall Statistics:\")\n",
    "#         print(f\"  • Total frames processed: {len(df):,}\")\n",
    "#         print(f\"  • Collapsed frames: {df['is_pocket_collapse'].sum():,} \"\n",
    "#               f\"({df['is_pocket_collapse'].mean():.1%})\")\n",
    "#         print(f\"  • Clean pocket frames: {(~df['is_pocket_collapse']).sum():,} \"\n",
    "#               f\"({(~df['is_pocket_collapse']).mean():.1%})\")\n",
    "        \n",
    "#         print(f\"\\nSeverity Distribution:\")\n",
    "#         severity_counts = df['collapse_severity'].value_counts()\n",
    "#         for severity in ['Critical', 'Severe', 'Moderate', 'Light', 'None']:\n",
    "#             count = severity_counts.get(severity, 0)\n",
    "#             pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "#             print(f\"  • {severity:12s}: {count:5,} frames ({pct:5.1f}%)\")\n",
    "        \n",
    "#         print(f\"\\nContributing Factors (frames triggered by each condition):\")\n",
    "#         print(f\"  • Separation + Convergence: {df['collapse_factor_separation'].sum():,} frames\")\n",
    "#         print(f\"  • High Convergence (Swarm): {df['collapse_factor_convergence'].sum():,} frames\")\n",
    "#         print(f\"  • Immediate Threat (3yds): {df['collapse_factor_immediate_threat'].sum():,} frames\")\n",
    "#         print(f\"  • Velocity-based Closure: {df['collapse_factor_velocity'].sum():,} frames\")\n",
    "        \n",
    "#         print(f\"\\nNew Columns Added:\")\n",
    "#         new_cols = [\n",
    "#             'collapse_factor_separation',\n",
    "#             'collapse_factor_convergence',\n",
    "#             'collapse_factor_immediate_threat',\n",
    "#             'collapse_factor_velocity',\n",
    "#             'is_pocket_collapse',\n",
    "#             'collapse_severity'\n",
    "#         ]\n",
    "#         for col in new_cols:\n",
    "#             print(f\"  • {col}\")\n",
    "        \n",
    "#         print(\"=\"*70)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cfa6a5",
   "metadata": {},
   "source": [
    "#### QB Merge Frame Level- Separation, Pressure Velocity, Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9640ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_enhanced_collapse_indicator(\n",
    "#     merged_df: pd.DataFrame,\n",
    "#     config: Optional[Dict] = None,\n",
    "#     verbose: bool = True\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Create enhanced pocket collapse indicator using multi-dimensional criteria.\n",
    "    \n",
    "#     Applies sophisticated collapse detection logic that combines QB separation,\n",
    "#     defender convergence, and velocity metrics. Produces both binary collapse\n",
    "#     flags and severity classifications for each frame.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     merged_df : pd.DataFrame\n",
    "#         Frame-level data with both QB separation and convergence metrics.\n",
    "        \n",
    "#         Required columns:\n",
    "#         - qb_min_separation : float\n",
    "#         - defenders_immediate_zone : int\n",
    "#         - defenders_closing_zone : int\n",
    "#         - defenders_potential_zone : int\n",
    "        \n",
    "#         Optional columns:\n",
    "#         - separation_velocity : float\n",
    "    \n",
    "#     config : Dict, optional\n",
    "#         Custom configuration for collapse thresholds.\n",
    "#         If None, uses ENHANCED_COLLAPSE_CONFIG constant.\n",
    "    \n",
    "#     verbose : bool, default=True\n",
    "#         Print collapse detection statistics and diagnostics.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Input dataframe with additional collapse-related columns:\n",
    "#         - collapse_factor_separation : bool\n",
    "#         - collapse_factor_convergence : bool\n",
    "#         - collapse_factor_immediate_threat : bool\n",
    "#         - collapse_factor_velocity : bool\n",
    "#         - is_pocket_collapse : bool\n",
    "#         - collapse_severity : str\n",
    "    \n",
    "#     Raises\n",
    "#     ------\n",
    "#     ValueError\n",
    "#         If required columns are missing from input DataFrame.\n",
    "#     TypeError\n",
    "#         If merged_df is not a pandas DataFrame.\n",
    "    \n",
    "#     Examples\n",
    "#     --------\n",
    "#     >>> enhanced_df = create_enhanced_collapse_indicator(merged_features)\n",
    "#     >>> collapse_rate = enhanced_df['is_pocket_collapse'].mean()\n",
    "#     >>> print(f\"Collapse rate: {collapse_rate:.1%}\")\n",
    "    \n",
    "#     Notes\n",
    "#     -----\n",
    "#     Enhanced Collapse Logic detects pocket collapse if ANY of these conditions are true:\n",
    "#     1. Separation + Convergence: QB separation < 7 yds AND 2+ defenders within 7-yard zone\n",
    "#     2. High Convergence (swarm): 3+ defenders within 5-yard zone\n",
    "#     3. Immediate Threat: 2+ defenders within 3-yard immediate zone\n",
    "#     4. Velocity + Convergence: Rapid closure (< -5 yds/sec) AND 1+ defenders within 5-yard zone\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # INPUT VALIDATION\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     # Check if input is None\n",
    "#     if merged_df is None:\n",
    "#         raise ValueError(\n",
    "#             \"Input DataFrame is None. \"\n",
    "#             \"Please ensure merged_pressure_features is created successfully.\"\n",
    "#         )\n",
    "    \n",
    "#     # Check if input is a DataFrame\n",
    "#     if not isinstance(merged_df, pd.DataFrame):\n",
    "#         raise TypeError(\n",
    "#             f\"Expected pandas DataFrame, got {type(merged_df).__name__}. \"\n",
    "#             f\"Please provide a valid merged DataFrame.\"\n",
    "#         )\n",
    "    \n",
    "#     # Check if DataFrame is empty\n",
    "#     if merged_df.empty:\n",
    "#         raise ValueError(\n",
    "#             \"Input DataFrame is empty. \"\n",
    "#             \"Please ensure data is loaded and merged correctly.\"\n",
    "#         )\n",
    "    \n",
    "#     # Define required columns\n",
    "#     required_columns = [\n",
    "#         'qb_min_separation',\n",
    "#         'defenders_immediate_zone',\n",
    "#         'defenders_closing_zone',\n",
    "#         'defenders_potential_zone'\n",
    "#     ]\n",
    "    \n",
    "#     # Check for missing columns\n",
    "#     missing_columns = [col for col in required_columns if col not in merged_df.columns]\n",
    "    \n",
    "#     if missing_columns:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"ERROR: MISSING REQUIRED COLUMNS\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"Missing columns: {missing_columns}\")\n",
    "#         print(f\"\\nAvailable columns in input DataFrame:\")\n",
    "#         print(merged_df.columns.tolist())\n",
    "#         print(\"=\"*70)\n",
    "#         raise ValueError(\n",
    "#             f\"Missing required columns: {missing_columns}\\n\"\n",
    "#             f\"Required columns: {required_columns}\\n\"\n",
    "#             f\"Please ensure you've merged QB separation with convergence metrics.\"\n",
    "#         )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONFIGURATION SETUP\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     # Use provided config or default\n",
    "#     if config is None:\n",
    "#         config = ENHANCED_COLLAPSE_CONFIG\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"CREATING ENHANCED POCKET COLLAPSE INDICATOR\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"Input DataFrame shape: {merged_df.shape}\")\n",
    "#         print(f\"Method: Multi-dimensional collapse detection\")\n",
    "#         print(f\"\\nConfiguration:\")\n",
    "#         for key, value in config.items():\n",
    "#             print(f\"  • {key}: {value}\")\n",
    "#         print(\"-\"*70)\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # DATA PREPARATION\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     try:\n",
    "#         # Create working copy to avoid modifying original data\n",
    "#         df = merged_df.copy()\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\nDataFrame copied successfully\")\n",
    "#             print(f\"Working with {len(df):,} frames\")\n",
    "#             print(f\"\\nAvailable columns: {df.columns.tolist()}\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"ERROR: Failed to copy DataFrame\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"Error type: {type(e).__name__}\")\n",
    "#         print(f\"Error message: {str(e)}\")\n",
    "#         print(\"=\"*70)\n",
    "#         raise\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # EXTRACT CONFIGURATION PARAMETERS\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     sep_threshold = config.get('separation_threshold', 7.0)\n",
    "#     critical_sep = config.get('critical_separation', 4.0)\n",
    "#     min_converging = config.get('min_converging_defenders', 2)\n",
    "#     critical_converging = config.get('critical_converging_defenders', 3)\n",
    "#     velocity_threshold = config.get('velocity_threshold', -5.0)\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(f\"\\nConfiguration extracted:\")\n",
    "#         print(f\"  • Separation threshold: {sep_threshold} yards\")\n",
    "#         print(f\"  • Critical separation: {critical_sep} yards\")\n",
    "#         print(f\"  • Min converging defenders: {min_converging}\")\n",
    "#         print(f\"  • Critical converging defenders: {critical_converging}\")\n",
    "#         print(f\"  • Velocity threshold: {velocity_threshold} yds/sec\")\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 1: SEPARATION + CONVERGENCE COLLAPSE\n",
    "#     # =========================================================================\n",
    "#     # QB separation low AND multiple defenders converging\n",
    "    \n",
    "#     df['collapse_factor_separation'] = (\n",
    "#         (df['qb_min_separation'] < sep_threshold) &\n",
    "#         (df['defenders_potential_zone'] >= min_converging)\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 2: HIGH CONVERGENCE (SWARM)\n",
    "#     # =========================================================================\n",
    "#     # 3+ defenders within 5-yard zone regardless of QB separation\n",
    "    \n",
    "#     df['collapse_factor_convergence'] = (\n",
    "#         df['defenders_closing_zone'] >= critical_converging\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 3: IMMEDIATE THREAT\n",
    "#     # =========================================================================\n",
    "#     # 2+ defenders within 3-yard immediate zone (imminent sack)\n",
    "    \n",
    "#     df['collapse_factor_immediate_threat'] = (\n",
    "#         df['defenders_immediate_zone'] >= 2\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # CONDITION 4: VELOCITY + CONVERGENCE (IF VELOCITY AVAILABLE)\n",
    "#     # =========================================================================\n",
    "#     # Rapid closure (velocity < -5 yds/sec) AND defenders within 5-yard zone\n",
    "    \n",
    "#     if 'separation_velocity' in df.columns:\n",
    "#         # Check for non-null velocity values\n",
    "#         has_velocity_data = df['separation_velocity'].notna().any()\n",
    "        \n",
    "#         if has_velocity_data:\n",
    "#             df['collapse_factor_velocity'] = (\n",
    "#                 (df['separation_velocity'] < velocity_threshold) &\n",
    "#                 (df['defenders_closing_zone'] >= 1)\n",
    "#             )\n",
    "#             if verbose:\n",
    "#                 print(f\"\\nVelocity-based detection enabled\")\n",
    "#                 print(f\"  • Frames with velocity data: {df['separation_velocity'].notna().sum():,}\")\n",
    "#         else:\n",
    "#             df['collapse_factor_velocity'] = False\n",
    "#             if verbose:\n",
    "#                 print(f\"\\nVelocity column exists but contains no valid data\")\n",
    "#     else:\n",
    "#         df['collapse_factor_velocity'] = False\n",
    "#         if verbose:\n",
    "#             print(f\"\\nVelocity-based detection disabled (no velocity column)\")\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # PRIMARY COLLAPSE INDICATOR (ANY CONDITION TRIGGERS COLLAPSE)\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     df['is_pocket_collapse'] = (\n",
    "#         df['collapse_factor_separation'] |\n",
    "#         df['collapse_factor_convergence'] |\n",
    "#         df['collapse_factor_immediate_threat'] |\n",
    "#         df['collapse_factor_velocity']\n",
    "#     )\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # COLLAPSE SEVERITY CLASSIFICATION\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     # Define severity conditions (evaluated in order of severity)\n",
    "#     conditions = [\n",
    "#         # Critical: 2+ defenders within 3 yards OR separation < 4 with 3+ converging\n",
    "#         (df['defenders_immediate_zone'] >= 2) | \n",
    "#         ((df['qb_min_separation'] < critical_sep) & \n",
    "#          (df['defenders_potential_zone'] >= critical_converging)),\n",
    "        \n",
    "#         # Severe: 3+ defenders within 5 yards OR separation < 5 with 2+ converging\n",
    "#         (df['defenders_closing_zone'] >= critical_converging) |\n",
    "#         ((df['qb_min_separation'] < 5.0) & \n",
    "#          (df['defenders_potential_zone'] >= min_converging)),\n",
    "        \n",
    "#         # Moderate: 2+ defenders within 7 yards AND separation < 7\n",
    "#         (df['defenders_potential_zone'] >= min_converging) & \n",
    "#         (df['qb_min_separation'] < sep_threshold),\n",
    "        \n",
    "#         # Light: Separation < 7 BUT only 0-1 converging defenders\n",
    "#         (df['qb_min_separation'] < sep_threshold) & \n",
    "#         (df['defenders_potential_zone'] < min_converging)\n",
    "#     ]\n",
    "    \n",
    "#     severity_levels = ['Critical', 'Severe', 'Moderate', 'Light']\n",
    "#     df['collapse_severity'] = np.select(conditions, severity_levels, default='None')\n",
    "    \n",
    "#     # =========================================================================\n",
    "#     # SUMMARY STATISTICS\n",
    "#     # =========================================================================\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(\"\\n\" + \"=\"*70)\n",
    "#         print(\"COLLAPSE DETECTION COMPLETE\")\n",
    "#         print(\"=\"*70)\n",
    "        \n",
    "#         print(f\"\\nOverall Statistics:\")\n",
    "#         print(f\"  • Total frames processed: {len(df):,}\")\n",
    "#         print(f\"  • Collapsed frames: {df['is_pocket_collapse'].sum():,} \"\n",
    "#               f\"({df['is_pocket_collapse'].mean():.1%})\")\n",
    "#         print(f\"  • Clean pocket frames: {(~df['is_pocket_collapse']).sum():,} \"\n",
    "#               f\"({(~df['is_pocket_collapse']).mean():.1%})\")\n",
    "        \n",
    "#         print(f\"\\nSeverity Distribution:\")\n",
    "#         severity_counts = df['collapse_severity'].value_counts()\n",
    "#         for severity in ['Critical', 'Severe', 'Moderate', 'Light', 'None']:\n",
    "#             count = severity_counts.get(severity, 0)\n",
    "#             pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "#             print(f\"  • {severity:12s}: {count:5,} frames ({pct:5.1f}%)\")\n",
    "        \n",
    "#         print(f\"\\nContributing Factors (frames triggered by each condition):\")\n",
    "#         print(f\"  • Separation + Convergence: {df['collapse_factor_separation'].sum():,} frames\")\n",
    "#         print(f\"  • High Convergence (Swarm): {df['collapse_factor_convergence'].sum():,} frames\")\n",
    "#         print(f\"  • Immediate Threat (3yds): {df['collapse_factor_immediate_threat'].sum():,} frames\")\n",
    "#         print(f\"  • Velocity-based Closure: {df['collapse_factor_velocity'].sum():,} frames\")\n",
    "        \n",
    "#         print(f\"\\nNew Columns Added:\")\n",
    "#         new_cols = [\n",
    "#             'collapse_factor_separation',\n",
    "#             'collapse_factor_convergence',\n",
    "#             'collapse_factor_immediate_threat',\n",
    "#             'collapse_factor_velocity',\n",
    "#             'is_pocket_collapse',\n",
    "#             'collapse_severity'\n",
    "#         ]\n",
    "#         for col in new_cols:\n",
    "#             print(f\"  • {col}\")\n",
    "        \n",
    "#         print(\"=\"*70)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4402a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e218f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION 1: MERGE QB SEPARATION WITH CONVERGENCE\n",
    "# ============================================================================\n",
    "\n",
    "def merge_qb_separation_with_convergence(\n",
    "    qb_separation_df: pd.DataFrame,\n",
    "    convergence_df: pd.DataFrame,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge frame-level QB separation data with defender convergence metrics.\n",
    "    \n",
    "    Combines QB separation metrics (distance to nearest defender, velocity)\n",
    "    with defender convergence metrics (zone counts, convergence category)\n",
    "    at frame granularity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    qb_separation_df : pd.DataFrame\n",
    "        Frame-level QB separation and velocity data.\n",
    "        \n",
    "        Required columns:\n",
    "        - game_id : int\n",
    "            Unique game identifier\n",
    "        - play_id : int\n",
    "            Unique play identifier\n",
    "        - frame_id : int\n",
    "            Frame number (10 fps)\n",
    "        - nfl_id : int\n",
    "            Quarterback NFL ID\n",
    "        - qb_min_separation : float\n",
    "            Distance to nearest defender (yards)\n",
    "        - separation_velocity : float\n",
    "            Rate of separation change (yards/second)\n",
    "        - pressure_acceleration : float, optional\n",
    "            Rate of velocity change (yards/second²)\n",
    "        \n",
    "        Optional columns:\n",
    "        - player_name, player_role, player_position\n",
    "    \n",
    "    convergence_df : pd.DataFrame\n",
    "        Frame-level defender convergence data from Step A.2.\n",
    "        \n",
    "        Required columns:\n",
    "        - game_id, play_id, frame_id, nfl_id : identifiers\n",
    "        - defenders_immediate_zone : int\n",
    "            Count in immediate threat zone (0-3 yards)\n",
    "        - defenders_closing_zone : int\n",
    "            Count in closing threat zone (0-5 yards)\n",
    "        - defenders_potential_zone : int\n",
    "            Count in potential threat zone (0-7 yards)\n",
    "        - total_converging_defenders : int\n",
    "            Total defenders within 7-yard zone\n",
    "        - convergence_category : str\n",
    "            'Critical', 'High', 'Moderate', or 'None'\n",
    "        - closest_defender_distance : float\n",
    "            Distance to nearest defender (for validation)\n",
    "        - total_defenders_on_field : int\n",
    "            Total defenders in frame\n",
    "    \n",
    "    verbose : bool, default=True\n",
    "        Print merge statistics and diagnostics.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Merged frame-level data with both QB separation and convergence metrics.\n",
    "        \n",
    "        Output columns include all columns from both input dataframes.\n",
    "        Merge key: (game_id, play_id, frame_id, nfl_id)\n",
    "        \n",
    "        Note: Does NOT include collapse indicators - those are created in\n",
    "        the next pipeline step.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If merge produces unexpected number of rows or low match rate.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    **Merge Strategy:**\n",
    "    - Type: LEFT join (preserve all QB frames)\n",
    "    - Key: (game_id, play_id, frame_id, nfl_id)\n",
    "    - Missing convergence data filled with zeros (frames without defenders)\n",
    "    \n",
    "    **Expected Match Rate:**\n",
    "    Should be ~100% if both calculations used same input_data.\n",
    "    Lower match rate may indicate:\n",
    "    - Frames without defenders (filled with zeros)\n",
    "    - Data inconsistencies between sources\n",
    "    \n",
    "    **Pipeline Position:**\n",
    "    1. calculate_frame_level_separation_qb() → QB separation\n",
    "    2. calculate_qb_pressure_velocity() → Add velocity\n",
    "    3. calculate_frame_level_defender_convergence() → Convergence metrics\n",
    "    4. merge_qb_separation_with_convergence() → Combined features ← THIS\n",
    "    \n",
    "    **Data Quality:**\n",
    "    - Fills missing convergence with zeros (safe assumption: no defenders)\n",
    "    - Validates match rate (warns if < 95%)\n",
    "    - Preserves temporal continuity (all QB frames retained)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage\n",
    "    >>> merged_df = merge_qb_separation_with_convergence(\n",
    "    ...     qb_velocity_frame_level,\n",
    "    ...     convergence_frame_level\n",
    "    ... )\n",
    "    >>> \n",
    "    >>> # Check merge quality\n",
    "    >>> print(f\"Shape: {merged_df.shape}\")\n",
    "    >>> print(f\"Columns: {merged_df.columns.tolist()}\")\n",
    "    >>> \n",
    "    >>> # Verify no collapse indicators yet\n",
    "    >>> assert 'is_pocket_collapse' not in merged_df.columns\n",
    "    >>> # Collapse indicators created in next step\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    calculate_qb_pressure_velocity : Creates velocity features\n",
    "    calculate_frame_level_defender_convergence : Creates convergence features\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"MERGING QB SEPARATION WITH CONVERGENCE (FRAME-LEVEL)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"QB separation data: {qb_separation_df.shape[0]:,} frames\")\n",
    "        print(f\"Convergence data: {convergence_df.shape[0]:,} frames\")\n",
    "        print(\"\\nPurpose: Combine pressure metrics for collapse detection\")\n",
    "        print(\"Note: Collapse indicators created in next step\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # COLUMN SELECTION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Select convergence columns to merge (avoid duplicates)\n",
    "    convergence_cols_to_merge = [\n",
    "        'game_id', 'play_id', 'frame_id', 'nfl_id',\n",
    "        'defenders_immediate_zone',\n",
    "        'defenders_closing_zone',\n",
    "        'defenders_potential_zone',\n",
    "        'total_converging_defenders',\n",
    "        'convergence_category',\n",
    "        'total_defenders_on_field'\n",
    "        # Exclude 'closest_defender_distance' - it's duplicate of qb_min_separation\n",
    "    ]\n",
    "    \n",
    "    # Filter to available columns\n",
    "    convergence_available_cols = [\n",
    "        col for col in convergence_cols_to_merge \n",
    "        if col in convergence_df.columns\n",
    "    ]\n",
    "\n",
    "    # Filter to available columns\n",
    "    available_cols = [col for col in convergence_cols_to_merge \n",
    "                      if col in convergence_df.columns]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # MERGE DATAFRAMES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Perform left join (keep all QB frames)\n",
    "    merged_df = qb_separation_df.merge(\n",
    "        convergence_df[convergence_available_cols],\n",
    "        on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "        how='left',\n",
    "        suffixes=('', '_conv')  # Handle any column overlaps\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # HANDLE MISSING VALUES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Fill missing convergence data with zeros\n",
    "    # (Frames without defenders = zero convergence)\n",
    "    convergence_numeric_cols = [\n",
    "        'defenders_immediate_zone',\n",
    "        'defenders_closing_zone',\n",
    "        'defenders_potential_zone',\n",
    "        'total_converging_defenders',\n",
    "        'total_defenders_on_field'\n",
    "    ]\n",
    "    \n",
    "    for col in convergence_numeric_cols:\n",
    "        if col in merged_df.columns:\n",
    "            merged_df[col] = merged_df[col].fillna(0).astype(int)\n",
    "    \n",
    "    # Fill missing convergence category\n",
    "    if 'convergence_category' in merged_df.columns:\n",
    "        merged_df['convergence_category'] = merged_df['convergence_category'].fillna('None')\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # MERGE QUALITY VALIDATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        # Calculate match rate\n",
    "        matched_frames = (\n",
    "            (~merged_df['defenders_potential_zone'].isna()).sum() \n",
    "            if 'defenders_potential_zone' in merged_df.columns \n",
    "            else merged_df['convergence_category'].notna().sum()\n",
    "        )\n",
    "        match_rate = (matched_frames / len(merged_df) * 100) if len(merged_df) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nMerge Results:\")\n",
    "        print(f\"  • Total frames after merge: {len(merged_df):,}\")\n",
    "        print(f\"  • Frames with convergence data: {matched_frames:,}\")\n",
    "        print(f\"  • Match rate: {match_rate:.1f}%\")\n",
    "        print(f\"  • Total columns: {merged_df.shape[1]}\")\n",
    "        \n",
    "        # Validation warnings\n",
    "        if match_rate < 95:\n",
    "            print(f\"\\n  ⚠ Warning: Low match rate ({match_rate:.1f}%)\")\n",
    "            print(f\"     Expected ~100% if same input_data used\")\n",
    "            print(f\"     Missing data filled with zeros (safe for frames without defenders)\")\n",
    "        else:\n",
    "            print(f\"  ✓ Good match rate - data sources aligned\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b78c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_qb_frame_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Organize QB frame-level columns in logical order.\"\"\"\n",
    "    \n",
    "    # Define column order\n",
    "    column_order = [\n",
    "        # Identifiers\n",
    "        'game_id', 'play_id', 'nfl_id', 'frame_id',\n",
    "        \n",
    "        # Player info\n",
    "        'player_name', 'player_role', 'player_position',\n",
    "        \n",
    "        # QB Separation metrics\n",
    "        'qb_min_separation',\n",
    "        'separation_velocity', \n",
    "        'pressure_acceleration',\n",
    "        \n",
    "        # Defender convergence counts\n",
    "        'defenders_immediate_zone',\n",
    "        'defenders_closing_zone',\n",
    "        'defenders_potential_zone', \n",
    "        'total_converging_defenders',\n",
    "        'convergence_category',\n",
    "        'total_defenders_on_field',\n",
    "        \n",
    "        # Collapse indicators\n",
    "        'collapse_factor_separation',\n",
    "        'collapse_factor_convergence',\n",
    "        'collapse_factor_immediate_threat',\n",
    "        'collapse_factor_velocity',\n",
    "        'is_pocket_collapse',\n",
    "        'collapse_severity'\n",
    "    ]\n",
    "    \n",
    "    # Select available columns in order\n",
    "    available_cols = [col for col in column_order if col in df.columns]\n",
    "    \n",
    "    # Add any remaining columns\n",
    "    remaining_cols = [col for col in df.columns if col not in available_cols]\n",
    "    final_cols = available_cols + remaining_cols\n",
    "    \n",
    "    return df[final_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace9fcd1",
   "metadata": {},
   "source": [
    "### Frame Level - TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec0e089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frame_level_separation_tr(df):\n",
    "    \"\"\"\n",
    "    Calculate minimum separation between Targeted Receiver and defensive players at FRAME level.\n",
    "    Returns only Targeted Receiver data with tr_min_separation per frame.\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame containing tracking data with columns:\n",
    "        - game_id, play_id, frame_id, nfl_id\n",
    "        - player_role: 'Targeted Receiver' or 'Defensive Coverage'\n",
    "        - player_name\n",
    "        - x, y: Player coordinates\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Targeted Receiver data with columns (game_id, play_id, nfl_id, frame_id, player_name, player_role, tr_min_separation)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by game, play, and frame (FRAME-LEVEL calculation)\n",
    "    grouped = df.groupby(['game_id', 'play_id', 'frame_id'])\n",
    "    \n",
    "    # List to store Targeted Receiver results\n",
    "    tr_results = []\n",
    "    \n",
    "    for (game_id, play_id, frame_id), frame_group in grouped:\n",
    "        # Identify Targeted Receivers and defenders in this specific frame\n",
    "        target_receivers = frame_group[frame_group['player_role'] == 'Targeted Receiver']\n",
    "        defenders = frame_group[frame_group['player_role'] == 'Defensive Coverage']\n",
    "        \n",
    "        # Skip if no Targeted Receivers or defenders in this frame\n",
    "        if target_receivers.empty or defenders.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extract defender coordinates as arrays for vectorized calculation\n",
    "        def_x = defenders['x'].values\n",
    "        def_y = defenders['y'].values\n",
    "        \n",
    "        # For each Targeted Receiver, calculate distance to all defenders using vectorization\n",
    "        for tr_idx, tr_row in target_receivers.iterrows():\n",
    "            tr_x, tr_y = tr_row['x'], tr_row['y']\n",
    "            \n",
    "            # Vectorized distance calculation across all defenders in this frame\n",
    "            distances = calculate_euclidean_distance_vectorized(tr_x, tr_y, def_x, def_y)\n",
    "            \n",
    "            # Find minimum distance for this frame\n",
    "            min_distance = np.round(np.min(distances), 2)\n",
    "            \n",
    "            # Create result record for this Targeted Receiver\n",
    "            tr_result = {\n",
    "                'game_id': game_id,\n",
    "                'play_id': play_id,\n",
    "                'nfl_id': tr_row['nfl_id'],\n",
    "                'frame_id': frame_id,\n",
    "                'player_name': tr_row['player_name'],\n",
    "                'player_role': tr_row['player_role'],\n",
    "                'player_position': tr_row['player_position'],\n",
    "                'tr_min_separation': min_distance,\n",
    "                'route_of_targeted_receiver': tr_row['route_of_targeted_receiver'],\n",
    "                'dir': tr_row['dir']\n",
    "            }\n",
    "            tr_results.append(tr_result)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(tr_results)\n",
    "    \n",
    "    # Sort by game, play, receiver, and frame for chronological analysis\n",
    "    if not result_df.empty:\n",
    "        result_df = result_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05706d5a",
   "metadata": {},
   "source": [
    "### Play Level Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad2b72",
   "metadata": {},
   "source": [
    "#### PLay Level - QB Separation, Pressure Velocity, Convergence and Pocket Collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7d48a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QB PLAY-LEVEL AGGREGATION\n",
    "# ============================================================================\n",
    "# Project: NFL Passing Game Analysis\n",
    "# Author: Hsotuhsa-S\n",
    "# Date: 2025-11-19 23:35:15\n",
    "# Purpose: Aggregate complete QB frame-level features to play-level\n",
    "# Version: Corrected - Pure aggregation, no feature engineering\n",
    "#\n",
    "# Design Principle:\n",
    "#   - Input: Complete frame-level features (already calculated)\n",
    "#   - Process: GROUP BY play, AGGREGATE with statistical functions\n",
    "#   - Output: Play-level summary statistics\n",
    "#   \n",
    "#\n",
    "# Dependencies:\n",
    "#   - pandas >= 1.3.0\n",
    "#   - numpy >= 1.21.0\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def aggregate_qb_features_to_play_level(\n",
    "    frame_df: pd.DataFrame,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate QB frame-level features to play-level using pandas groupby.\n",
    "    \n",
    "    This is PURE AGGREGATION - no feature engineering, no distance calculations,\n",
    "    no loops. Simply summarizes existing frame-level metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frame_df : pd.DataFrame\n",
    "        Complete frame-level QB data with ALL features already calculated.\n",
    "        \n",
    "        Expected input columns (from previous steps):\n",
    "        - game_id, play_id, nfl_id, frame_id\n",
    "        - qb_min_separation (from calculate_frame_level_separation_qb)\n",
    "        - separation_velocity (from calculate_qb_pressure_velocity)\n",
    "        - defenders_immediate_zone, defenders_closing_zone, defenders_potential_zone\n",
    "          (from calculate_frame_level_defender_convergence)\n",
    "        - total_converging_defenders (from calculate_frame_level_defender_convergence)\n",
    "        - closest_defender_distance (from calculate_frame_level_defender_convergence)\n",
    "        - player_name, player_role (metadata)\n",
    "    \n",
    "    verbose : bool, default=True\n",
    "        Print aggregation summary.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Play-level aggregated QB features.\n",
    "        \n",
    "        Output columns (~25-30):\n",
    "        - game_id, play_id, nfl_id (identifiers)\n",
    "        - qb_play_min_separation, qb_play_avg_separation, qb_play_var_separation\n",
    "        - max_pressure_velocity, pressure_velocity_avg, pressure_volatility\n",
    "        - max_converging_defenders, avg_converging_defenders, std_converging_defenders\n",
    "        - max_defenders_immediate_zone, max_defenders_closing_zone\n",
    "        - time_to_throw, total_frames\n",
    "        - (and more...)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    **This Function Does:**\n",
    "    - Groups frame-level data by (game_id, play_id, nfl_id)\n",
    "    - Applies aggregation functions (MIN, MAX, MEAN, STD, SUM, COUNT)\n",
    "    - Creates play-level summary statistics\n",
    "    \n",
    "    **This Function Does NOT:**\n",
    "    - Calculate distances (already done in frame-level functions)\n",
    "    - Loop through rows (uses vectorized pandas operations)\n",
    "    - Create new features (only aggregates existing ones)\n",
    "    - Filter or transform data (pure aggregation)\n",
    "    \n",
    "    **Why This is Correct:**\n",
    "    Aggregation should be:\n",
    "    1. Simple: Just groupby + agg\n",
    "    2. Fast: Vectorized pandas operations\n",
    "    3. Clear: Obvious what it does\n",
    "    4. Separation of concerns: Aggregation ≠ Feature Engineering\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Input: Frame-level features (already calculated)\n",
    "    >>> print(enhanced_collapse_frame_level.shape)\n",
    "    (23103, 24)  # 23K frames with complete features\n",
    "    >>> \n",
    "    >>> # Aggregate to play-level\n",
    "    >>> qb_play = aggregate_qb_features_to_play_level(\n",
    "    ...     enhanced_collapse_frame_level\n",
    "    ... )\n",
    "    >>> \n",
    "    >>> # Output: Play-level aggregates\n",
    "    >>> print(qb_play.shape)\n",
    "    (818, 28)  # 818 plays with aggregated features\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"AGGREGATING QB FRAME-LEVEL FEATURES TO PLAY-LEVEL\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Input: {frame_df.shape[0]:,} frames\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # DEFINE AGGREGATION DICTIONARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Simple, declarative aggregation - no calculations, no loops\n",
    "    \n",
    "    agg_dict = {}\n",
    "    \n",
    "    # Separation metrics (if available)\n",
    "    if 'qb_min_separation' in frame_df.columns:\n",
    "        agg_dict['qb_min_separation'] = [\n",
    "            ('qb_play_min_separation', 'min'),      # Peak pressure\n",
    "            ('qb_play_avg_separation', 'mean'),     # Sustained pressure\n",
    "            ('qb_play_var_separation', 'var')       # Volatility\n",
    "        ]\n",
    "    \n",
    "    # Velocity metrics (if available)\n",
    "    if 'separation_velocity' in frame_df.columns:\n",
    "        agg_dict['separation_velocity'] = [\n",
    "            ('max_pressure_velocity', 'min'),       # Most negative\n",
    "            ('pressure_velocity_avg', 'mean'),\n",
    "            ('pressure_volatility', 'std')\n",
    "        ]\n",
    "    \n",
    "    # Acceleration metrics (if available)\n",
    "    if 'pressure_acceleration' in frame_df.columns:\n",
    "        agg_dict['pressure_acceleration'] = [\n",
    "            ('max_pressure_acceleration', 'min'),\n",
    "            ('pressure_acceleration_avg', 'mean')\n",
    "        ]\n",
    "    \n",
    "    # Convergence metrics (if available)\n",
    "    if 'total_converging_defenders' in frame_df.columns:\n",
    "        agg_dict['total_converging_defenders'] = [\n",
    "            ('max_converging_defenders', 'max'),\n",
    "            ('avg_converging_defenders', 'mean'),\n",
    "            ('std_converging_defenders', 'std')\n",
    "        ]\n",
    "    \n",
    "    if 'defenders_immediate_zone' in frame_df.columns:\n",
    "        agg_dict['defenders_immediate_zone'] = [\n",
    "            ('max_defenders_immediate_zone', 'max')\n",
    "        ]\n",
    "    \n",
    "    if 'defenders_closing_zone' in frame_df.columns:\n",
    "        agg_dict['defenders_closing_zone'] = [\n",
    "            ('max_defenders_closing_zone', 'max'),\n",
    "            ('avg_defenders_closing_zone', 'mean')\n",
    "        ]\n",
    "    \n",
    "    if 'defenders_potential_zone' in frame_df.columns:\n",
    "        agg_dict['defenders_potential_zone'] = [\n",
    "            ('max_defenders_potential_zone', 'max')\n",
    "        ]\n",
    "    \n",
    "    # Player metadata\n",
    "    if 'player_name' in frame_df.columns:\n",
    "        agg_dict['player_name'] = [('player_name', 'first')]\n",
    "    \n",
    "    if 'player_role' in frame_df.columns:\n",
    "        agg_dict['player_role'] = [('player_role', 'first')]\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PERFORM AGGREGATION (Simple groupby)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    play_agg = frame_df.groupby(['game_id', 'play_id', 'nfl_id']).agg(agg_dict)\n",
    "    \n",
    "    \n",
    "    # ✅ CORRECT: Extract only the new column names\n",
    "    play_agg.columns = play_agg.columns.get_level_values(1)\n",
    "    play_agg = play_agg.reset_index()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"✓ Aggregated to {len(play_agg):,} rows\")\n",
    "        print(f\"✓ Features: {len(play_agg.columns)} columns\")\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # FILL MISSING VALUES\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Fill NaN standard deviations\n",
    "    std_cols = [col for col in play_agg.columns if 'std_' in col or '_volatility' in col]\n",
    "    for col in std_cols:\n",
    "        play_agg[col] = play_agg[col].fillna(0)\n",
    "    \n",
    "    # Fill NaN counts\n",
    "    count_cols = [col for col in play_agg.columns if 'count' in col.lower() or 'frames' in col.lower()]\n",
    "    for col in count_cols:\n",
    "        if col in play_agg.columns:\n",
    "            play_agg[col] = play_agg[col].fillna(0).astype(int)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SUMMARY\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Output: {play_agg.shape[0]:,} plays × {play_agg.shape[1]} features\")\n",
    "        print(f\"Reduction: {frame_df.shape[0] / play_agg.shape[0]:.1f}:1 (frames to plays)\")\n",
    "        \n",
    "        feature_groups = {\n",
    "            'Separation': [c for c in play_agg if 'separation' in c.lower()],\n",
    "            'Velocity': [c for c in play_agg if 'velocity' in c.lower() or 'acceleration' in c.lower()],\n",
    "            'Convergence': [c for c in play_agg if 'defender' in c.lower() or 'converg' in c.lower()],\n",
    "            #'Collapse': [c for c in play_agg if 'collapse' in c.lower()],\n",
    "            'Temporal': [c for c in play_agg if 'frame' in c.lower() or 'time' in c.lower()]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nFeature Groups:\")\n",
    "        for group, features in feature_groups.items():\n",
    "            if features:\n",
    "                print(f\"  • {group:15s}: {len(features):2d} features\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return play_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49241b",
   "metadata": {},
   "source": [
    "#### Play Level - TR Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a19503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_calculate_play_level_aggregates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate TR frame-level metrics to play-level statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Frame-level TR separation data with columns:\n",
    "        - game_id, play_id, nfl_id, frame_id\n",
    "        - tr_min_separation : float\n",
    "        - route_of_targeted_receiver : str\n",
    "        - player_name, player_role, player_position\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Play-level TR aggregates.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    **TR-Specific Aggregation Strategy:**\n",
    "    \n",
    "    - FIRST/LAST: Capture separation at snap and throw\n",
    "    - MIN: Tightest coverage moment\n",
    "    - MEAN: Overall coverage quality\n",
    "    - VAR: Coverage consistency (defender tracking)\n",
    "    \n",
    "    **Use Cases:**\n",
    "    - Route effectiveness analysis\n",
    "    - Coverage quality assessment\n",
    "    - QB decision evaluation\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> tr_play_agg = tr_calculate_play_level_aggregates(tr_separation_df)\n",
    "    >>> \n",
    "    >>> # Analyze route effectiveness\n",
    "    >>> route_analysis = tr_play_agg.groupby('route_of_targeted_receiver').agg({\n",
    "    ...     'tr_play_avg_separation': 'mean',\n",
    "    ...     'tr_play_min_separation': 'mean'\n",
    "    ... })\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to Targeted Receivers only\n",
    "    tr_data = df[df['player_role'] == 'Targeted Receiver'].copy()\n",
    "    \n",
    "    # Remove rows with missing separation\n",
    "    # tr_data = tr_data[tr_data['tr_min_separation'].notna()]\n",
    "    \n",
    "    if tr_data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # AGGREGATION FUNCTIONS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    play_aggregates = tr_data.groupby(['game_id', 'play_id', 'nfl_id']).agg({\n",
    "        # Separation metrics\n",
    "        'tr_min_separation': [\n",
    "            ('tr_play_min_separation', 'min'),      # Tightest coverage\n",
    "            ('tr_play_avg_separation', 'mean'),     # Average coverage\n",
    "            ('tr_play_var_separation', 'var')       # Coverage consistency\n",
    "        ],\n",
    "        \n",
    "        # Player information\n",
    "        'player_name': [('player_name', 'first')],\n",
    "        'player_role': [('player_role', 'first')],\n",
    "        'player_position': [('player_position', 'first')],\n",
    "        'route_of_targeted_receiver': [('route_of_targeted_receiver', 'first')],\n",
    "        \n",
    "        # Temporal features\n",
    "        'frame_id': [\n",
    "            ('nums_frame_pre_throw', 'count')       # Route duration\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Flatten column names\n",
    "\n",
    "     # ✅ CORRECT: Extract only the new column names\n",
    "    play_aggregates.columns = play_aggregates.columns.get_level_values(1)\n",
    "    play_aggregates = play_aggregates.reset_index(drop=False)\n",
    "\n",
    "    # Calculate time to throw from frame count\n",
    "    play_aggregates['time_to_throw'] = (play_aggregates['nums_frame_pre_throw'] * 0.1).round(2)\n",
    "\n",
    "    # Print shape of aggregated DataFrame\n",
    "    print(f\"TR Play-level aggregates shape: {play_aggregates.shape}\")\n",
    "\n",
    "    # Add rounding for separation metrics to ensure consistent 2 decimal places\n",
    "    separation_cols = ['tr_play_min_separation', 'tr_play_avg_separation', 'tr_play_var_separation']\n",
    "    for col in separation_cols:\n",
    "        if col in play_aggregates.columns:\n",
    "            play_aggregates[col] = play_aggregates[col].round(2)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # FIRST AND LAST FRAME SEPARATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Get first frame data (at snap)\n",
    "    first_frames = tr_data.sort_values('frame_id').groupby(\n",
    "        ['game_id', 'play_id', 'nfl_id'], as_index=False\n",
    "    ).first().reset_index()\n",
    "    \n",
    "    # Get last frame data (at throw)\n",
    "    last_frames = tr_data.sort_values('frame_id').groupby(\n",
    "        ['game_id', 'play_id', 'nfl_id'], as_index=False\n",
    "    ).last().reset_index()\n",
    "    \n",
    "    # Rename direction column in last frame\n",
    "    last_frames = last_frames.rename(columns={'dir': 'tr_last_dir'})\n",
    "    \n",
    "    # Merge first and last frame data\n",
    "    first_last = pd.merge(\n",
    "        first_frames[['game_id', 'play_id', 'nfl_id', 'tr_min_separation']],\n",
    "        last_frames[['game_id', 'play_id', 'nfl_id', 'tr_min_separation', 'tr_last_dir']],\n",
    "        on=['game_id', 'play_id', 'nfl_id'],\n",
    "        suffixes=('_first', '_last')\n",
    "    )\n",
    "    \n",
    "    # Merge with play aggregates\n",
    "    play_aggregates = pd.merge(\n",
    "        play_aggregates,\n",
    "        first_last,\n",
    "        on=['game_id', 'play_id', 'nfl_id'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return play_aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00957d1",
   "metadata": {},
   "source": [
    "#### Play Level - QB + TR + Supplementary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a427dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_play_level_features(\n",
    "    qb_play_enhanced: pd.DataFrame,\n",
    "    tr_play_agg: pd.DataFrame,\n",
    "    supplementary_data: pd.DataFrame,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    CORRECTED: Keep only plays that have actual tracking data (QB + TR).\n",
    "    Drop supplementary plays that don't exist in tracking datasets.\n",
    "    \n",
    "    Strategy: Inner joins to ensure all plays have complete tracking metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CORRECTED: TRACKING DATA ONLY - DROP INCOMPLETE PLAYS\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Strategy: Inner joins - only keep plays with QB AND TR tracking data\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: START WITH QB DATA (HAS ACTUAL PRESSURE METRICS)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    base_df = qb_play_enhanced.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[STEP 1] QB pressure data as base:\")\n",
    "        print(f\"  • QB plays with pressure metrics: {len(base_df):,}\")\n",
    "        print(f\"  • These have actual tracking-derived features\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: INNER JOIN WITH TR DATA (KEEP ONLY MATCHING PLAYS)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # TR columns to merge\n",
    "    tr_cols_to_merge = [\n",
    "        'game_id', 'play_id',\n",
    "        'tr_play_min_separation',\n",
    "        'tr_play_avg_separation', \n",
    "        'tr_play_var_separation',\n",
    "        'tr_min_separation_first',\n",
    "        'tr_min_separation_last',\n",
    "        'nums_frame_pre_throw',\n",
    "        'time_to_throw',\n",
    "        'route_of_targeted_receiver'\n",
    "    ]\n",
    "    \n",
    "    tr_available = [col for col in tr_cols_to_merge if col in tr_play_agg.columns]\n",
    "    \n",
    "    # INNER JOIN: Only keep plays that exist in BOTH QB and TR datasets\n",
    "    merged_qb_tr = base_df.merge(\n",
    "        tr_play_agg[tr_available],\n",
    "        on=['game_id', 'play_id'],\n",
    "        how='inner'  # ← KEY CHANGE: Only keep matching plays\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        qb_only_plays = len(base_df) - len(merged_qb_tr)\n",
    "        print(f\"\\n[STEP 2] After QB + TR inner join:\")\n",
    "        print(f\"  • Plays with BOTH QB and TR data: {len(merged_qb_tr):,}\")\n",
    "        print(f\"  • QB-only plays dropped: {qb_only_plays:,}\")\n",
    "        print(f\"  • Retention rate: {(len(merged_qb_tr)/len(base_df)*100):.1f}%\")\n",
    "        \n",
    "        # Verify time_to_throw preservation\n",
    "        if 'time_to_throw' in merged_qb_tr.columns:\n",
    "            missing_ttt = merged_qb_tr['time_to_throw'].isna().sum()\n",
    "            print(f\"  • time_to_throw missing: {missing_ttt} (should be 0)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: INNER JOIN WITH SUPPLEMENTARY (KEEP ONLY PLAYS WITH OUTCOMES)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    supp_cols = [\n",
    "        'game_id', 'play_id',\n",
    "        'pass_result',                  # TARGET VARIABLE\n",
    "        'pass_length', 'pass_type',\n",
    "        'yards_gained',\n",
    "        'pass_location_type',\n",
    "        'dropback_type', 'dropback_distance',\n",
    "        'play_action',\n",
    "        'offense_formation',\n",
    "        'defenders_in_the_box',\n",
    "        'team_coverage_man_zone',\n",
    "        'team_coverage_type'\n",
    "    ]\n",
    "    \n",
    "    available_supp_cols = [col for col in supp_cols if col in supplementary_data.columns]\n",
    "    \n",
    "    # INNER JOIN: Only keep plays that have outcomes\n",
    "    final_merged = merged_qb_tr.merge(\n",
    "        supplementary_data[available_supp_cols],\n",
    "        on=['game_id', 'play_id'],\n",
    "        how='inner'  # ← KEY CHANGE: Only keep plays with outcomes\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        tracking_only_plays = len(merged_qb_tr) - len(final_merged)\n",
    "        original_supp = len(supplementary_data)\n",
    "        \n",
    "        print(f\"\\n[STEP 3] After adding supplementary data (inner join):\")\n",
    "        print(f\"  • Final plays with complete data: {len(final_merged):,}\")\n",
    "        print(f\"  • Tracking-only plays dropped: {tracking_only_plays:,}\")\n",
    "        print(f\"  • Total supplementary plays: {original_supp:,}\")\n",
    "        print(f\"  • Supplementary plays used: {len(final_merged):,} ({len(final_merged)/original_supp*100:.1f}%)\")\n",
    "        \n",
    "        # Target variable check\n",
    "        target_available = (~final_merged['pass_result'].isna()).sum()\n",
    "        print(f\"  • Plays with target variable: {target_available:,}\")\n",
    "        \n",
    "        if target_available == len(final_merged):\n",
    "            print(f\"  ✓ All plays have target variable (perfect)\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Some plays missing target: {len(final_merged) - target_available}\")\n",
    "        \n",
    "        # Final data quality summary\n",
    "        print(f\"\\n[SUMMARY] Data Quality:\")\n",
    "        print(f\"  ✓ All plays have QB pressure metrics (tracking-derived)\")\n",
    "        print(f\"  ✓ All plays have TR coverage metrics (tracking-derived)\")\n",
    "        print(f\"  ✓ All plays have game context (supplementary)\")\n",
    "        print(f\"  ✓ No missing time_to_throw (from TR tracking)\")\n",
    "        print(f\"  ✓ High-quality dataset: {len(final_merged):,} complete plays\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return final_merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65dafa3",
   "metadata": {},
   "source": [
    "## Create pressure components score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "665b9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pressure_component_scores(\n",
    "    df: pd.DataFrame,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create individual pressure component scores per play.\n",
    "    \n",
    "    Creates three separate pressure scores using same logic as create_qb_pressure_index:\n",
    "    - convergence_pressure_score: Defender convergence (0-100)\n",
    "    - velocity_pressure_score: Velocity pressure (0-100) \n",
    "    - tr_coverage_pressure_score: TR separation pressure (0-100)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Play-level dataset with QB metrics\n",
    "    verbose : bool, default=True\n",
    "        Print detailed analysis\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataset with three new pressure component score columns\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"CREATING PRESSURE COMPONENT SCORES\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Components:\")\n",
    "        print(\"  1. Convergence Pressure: Defender convergence\")\n",
    "        print(\"  2. Velocity Pressure: Velocity pressure\")  \n",
    "        print(\"  3. TR Coverage Pressure: TR separation\")\n",
    "        print(\"-\"*70)\n",
    "    \n",
    "    df_scores = df.copy()\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. SPATIAL PRESSURE SCORE (Defender Convergence)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if 'max_converging_defenders' in df.columns:\n",
    "        convergence_raw = df_scores['max_converging_defenders'].fillna(0)\n",
    "        \n",
    "        # Use 95th percentile as scaling factor, minimum of 4 for normalization\n",
    "        p95 = np.percentile(convergence_raw[convergence_raw > 0], 95) if (convergence_raw > 0).any() else 4\n",
    "        convergence_pressure = np.clip(convergence_raw / max(p95, 1) * 100, 0, 100)\n",
    "        \n",
    "        df_scores['convergence_pressure_score'] = convergence_pressure.round(1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[CONVERGENCE PRESSURE]\")\n",
    "            print(f\"  • Source: max_converging_defenders\")\n",
    "            print(f\"  • Raw range: {convergence_raw.min():.1f} - {convergence_raw.max():.1f}\")\n",
    "            print(f\"  • 95th percentile: {p95:.1f}\")\n",
    "            print(f\"  • Score range: {convergence_pressure.min():.1f} - {convergence_pressure.max():.1f}\")\n",
    "            print(f\"  • Score mean: {convergence_pressure.mean():.1f}\")\n",
    "    else:\n",
    "        df_scores['convergence_pressure_score'] = 0.0\n",
    "        if verbose:\n",
    "            print(f\"[CONVERGENCE PRESSURE] ⚠ max_converging_defenders not found - using 0\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. VELOCITY PRESSURE SCORE (Temporal Pressure)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if 'max_pressure_velocity' in df.columns:\n",
    "        velocity_raw = df_scores['max_pressure_velocity'].fillna(0)\n",
    "        \n",
    "        # Clip velocity to reasonable range (-25 to 0), normalize to 0-100\n",
    "        velocity_clipped = np.clip(velocity_raw, -25, 0)\n",
    "        velocity_pressure = np.abs(velocity_clipped) / 15 * 100\n",
    "        velocity_pressure = np.clip(velocity_pressure, 0, 100)\n",
    "        \n",
    "        df_scores['velocity_pressure_score'] = velocity_pressure.round(1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[VELOCITY PRESSURE]\")\n",
    "            print(f\"  • Source: max_pressure_velocity\")\n",
    "            print(f\"  • Raw range: {velocity_raw.min():.1f} - {velocity_raw.max():.1f} yds/sec\")\n",
    "            print(f\"  • Clipped range: {velocity_clipped.min():.1f} - {velocity_clipped.max():.1f}\")\n",
    "            print(f\"  • Score range: {velocity_pressure.min():.1f} - {velocity_pressure.max():.1f}\")\n",
    "            print(f\"  • Score mean: {velocity_pressure.mean():.1f}\")\n",
    "    else:\n",
    "        df_scores['velocity_pressure_score'] = 0.0\n",
    "        if verbose:\n",
    "            print(f\"\\n[VELOCITY PRESSURE] ⚠ max_pressure_velocity not found - using 0\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. TR COVERAGE PRESSURE SCORE (TR Separation)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if 'tr_play_min_separation' in df.columns:\n",
    "        tr_sep_raw = df_scores['tr_play_min_separation'].fillna(8.0)\n",
    "        \n",
    "        # Inverse relationship: smaller separation = higher pressure\n",
    "        # Normalize to 0-10 yards range, then convert to 0-100 pressure scale\n",
    "        tr_coverage_pressure = (1 - np.clip(tr_sep_raw / 10.0, 0, 1)) * 100\n",
    "        \n",
    "        df_scores['tr_coverage_pressure_score'] = tr_coverage_pressure.round(1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[COVERAGE PRESSURE]\")\n",
    "            print(f\"  • Source: tr_play_min_separation\")\n",
    "            print(f\"  • Raw range: {tr_sep_raw.min():.1f} - {tr_sep_raw.max():.1f} yards\")\n",
    "            print(f\"  • Score range: {tr_coverage_pressure.min():.1f} - {tr_coverage_pressure.max():.1f}\")\n",
    "            print(f\"  • Score mean: {tr_coverage_pressure.mean():.1f}\")\n",
    "            print(f\"  • Logic: Lower separation = Higher pressure\")\n",
    "    else:\n",
    "        df_scores['tr_coverage_pressure_score'] = 0.0\n",
    "        if verbose:\n",
    "            print(f\"\\n[TR COVERAGE PRESSURE] ⚠ tr_play_min_separation not found - using 0\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SUMMARY STATISTICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(\"COMPONENT SCORES SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Component statistics\n",
    "        components = [\n",
    "            ('convergence_pressure_score', 'Spatial (Convergence)'),\n",
    "            ('velocity_pressure_score', 'Temporal (Velocity)'),\n",
    "            ('tr_coverage_pressure_score', 'Coverage (TR Sep)')\n",
    "        ]\n",
    "        \n",
    "        print(f\"Component Statistics:\")\n",
    "        for score_col, description in components:\n",
    "            if score_col in df_scores.columns:\n",
    "                mean_val = df_scores[score_col].mean()\n",
    "                std_val = df_scores[score_col].std()\n",
    "                min_val = df_scores[score_col].min()\n",
    "                max_val = df_scores[score_col].max()\n",
    "                \n",
    "                print(f\"  • {description:20s}: {mean_val:5.1f} ± {std_val:4.1f} (range: {min_val:4.1f}-{max_val:4.1f})\")\n",
    "        \n",
    "        # Component correlations\n",
    "        available_components = [comp[0] for comp in components if comp[0] in df_scores.columns]\n",
    "        if len(available_components) >= 2:\n",
    "            print(f\"\\nComponent Correlations:\")\n",
    "            for i, comp1 in enumerate(available_components):\n",
    "                for comp2 in available_components[i+1:]:\n",
    "                    corr = df_scores[comp1].corr(df_scores[comp2])\n",
    "                    comp1_name = next(desc for col, desc in components if col == comp1)\n",
    "                    comp2_name = next(desc for col, desc in components if col == comp2)\n",
    "                    print(f\"  • {comp1_name:20s} ↔ {comp2_name:20s}: {corr:+.3f}\")\n",
    "        \n",
    "        # Missing data check\n",
    "        missing_components = []\n",
    "        for score_col, description in components:\n",
    "            if score_col not in df_scores.columns or (df_scores[score_col] == 0).all():\n",
    "                missing_components.append(description)\n",
    "        \n",
    "        if missing_components:\n",
    "            print(f\"\\n⚠ Missing Components:\")\n",
    "            for comp in missing_components:\n",
    "                print(f\"    • {comp}\")\n",
    "        else:\n",
    "            print(f\"\\n✓ All components successfully calculated\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    return df_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91344bc5",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30e6d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION 4: VALIDATE FINAL FEATURES FOR ML\n",
    "# ============================================================================\n",
    "\n",
    "def validate_final_features(\n",
    "    final_df: pd.DataFrame,\n",
    "    target_column: str = 'pass_result'\n",
    ") -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Validate final feature set for ML pipeline readiness.\n",
    "    \n",
    "    Performs comprehensive data quality checks to ensure dataset meets\n",
    "    requirements for exploratory data analysis (EDA) and model training.\n",
    "    \n",
    "    This is the final quality gate before proceeding to the ML pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    final_df : pd.DataFrame\n",
    "        Final consolidated play-level feature set.\n",
    "        Output from merge_play_level_features().\n",
    "    \n",
    "    target_column : str, default='pass_result'\n",
    "        Name of target variable column.\n",
    "        Should contain binary classification labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, bool]\n",
    "        Validation results dictionary with boolean flags:\n",
    "        \n",
    "        - 'has_target_variable' : bool\n",
    "            Target column exists and has < 5% missing values\n",
    "        \n",
    "        - 'sufficient_samples' : bool\n",
    "            At least 100 samples (minimum for ML)\n",
    "        \n",
    "        - 'target_is_binary' : bool\n",
    "            Target has exactly 2 classes (binary classification)\n",
    "        \n",
    "        - 'balanced_target' : bool\n",
    "            Minority class ≥ 20% (reasonably balanced)\n",
    "        \n",
    "        - 'no_duplicate_rows' : bool\n",
    "            No duplicate play records exist\n",
    "        \n",
    "        - 'numeric_features_valid' : bool\n",
    "            No infinite values or excessive missing (> 50%)\n",
    "        \n",
    "        - 'ready_for_ml' : bool\n",
    "            Overall ML readiness (all critical checks passed)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    **Validation Criteria:**\n",
    "    \n",
    "    1. Target Variable Quality:\n",
    "       - Exists in dataframe\n",
    "       - < 5% missing values\n",
    "       - Exactly 2 unique classes (binary classification)\n",
    "       - Minority class ≥ 20% (avoid severe imbalance)\n",
    "    \n",
    "    2. Sample Size:\n",
    "       - Minimum: 100 samples (required)\n",
    "       - Recommended: 500+ samples (ideal for Random Forest)\n",
    "       - Current dataset: ~818 samples (✓ Good)\n",
    "    \n",
    "    3. Data Integrity:\n",
    "       - No duplicate play records\n",
    "       - No infinite values in numeric features\n",
    "       - < 50% missing in any feature column\n",
    "    \n",
    "    4. Feature Quality:\n",
    "       - At least 10 features available\n",
    "       - Mix of pressure, convergence, and context features\n",
    "       - Numeric features in valid ranges\n",
    "    \n",
    "    **Pass Criteria:**\n",
    "    All critical checks must pass for 'ready_for_ml' = True:\n",
    "    - has_target_variable\n",
    "    - sufficient_samples\n",
    "    - target_is_binary\n",
    "    - no_duplicate_rows\n",
    "    - numeric_features_valid\n",
    "    \n",
    "    Optional checks (warnings if fail):\n",
    "    - balanced_target (can proceed with class weights)\n",
    "    \n",
    "    **Next Steps Based on Results:**\n",
    "    \n",
    "    If ready_for_ml = True:\n",
    "    → Proceed to Data Cleaning & Preprocessing\n",
    "    → Exploratory Data Analysis (EDA)\n",
    "    → Feature Selection\n",
    "    → Model Training\n",
    "    \n",
    "    If ready_for_ml = False:\n",
    "    → Address failed checks\n",
    "    → Re-validate before proceeding\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Validate dataset\n",
    "    >>> validation = validate_final_features(final_ml_features)\n",
    "    >>> \n",
    "    >>> # Check results\n",
    "    >>> if validation['ready_for_ml']:\n",
    "    ...     print(\"✓ Dataset ready for ML pipeline\")\n",
    "    ...     print(\"→ Proceed to Data Cleaning\")\n",
    "    ... else:\n",
    "    ...     print(\"⚠ Dataset needs attention\")\n",
    "    ...     for check, passed in validation.items():\n",
    "    ...         if not passed:\n",
    "    ...             print(f\"  ✗ Failed: {check}\")\n",
    "    >>> \n",
    "    >>> # Detailed inspection\n",
    "    >>> print(f\"Sample size: {len(final_ml_features)}\")\n",
    "    >>> print(f\"Target balance: {final_ml_features[target_column].value_counts()}\")\n",
    "    >>> print(f\"Missing values: {final_ml_features.isnull().sum().sum()}\")\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    merge_play_level_features : Creates final feature set\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VALIDATING FINAL FEATURES FOR ML PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Target column: '{target_column}'\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CHECK 1: TARGET VARIABLE EXISTS AND POPULATED\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if target_column in final_df.columns:\n",
    "        target_missing = final_df[target_column].isna().sum()\n",
    "        target_missing_pct = (target_missing / len(final_df) * 100) if len(final_df) > 0 else 100\n",
    "        \n",
    "        validation_results['has_target_variable'] = (target_missing_pct < 5)\n",
    "        \n",
    "        if validation_results['has_target_variable']:\n",
    "            print(f\"✓ CHECK 1 PASSED: Target variable '{target_column}' available\")\n",
    "            print(f\"    Missing: {target_missing} ({target_missing_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"✗ CHECK 1 FAILED: Target variable has {target_missing_pct:.1f}% missing\")\n",
    "            print(f\"    Threshold: < 5% missing required\")\n",
    "    else:\n",
    "        validation_results['has_target_variable'] = False\n",
    "        print(f\"✗ CHECK 1 FAILED: Target column '{target_column}' not found\")\n",
    "        print(f\"    Available columns: {list(final_df.columns[:10])}...\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CHECK 2: SUFFICIENT SAMPLE SIZE\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    sample_count = len(final_df)\n",
    "    validation_results['sufficient_samples'] = (sample_count >= 100)\n",
    "    \n",
    "    if validation_results['sufficient_samples']:\n",
    "        print(f\"✓ CHECK 2 PASSED: Sufficient samples ({sample_count:,})\")\n",
    "        if sample_count >= 500:\n",
    "            print(f\"    → Excellent sample size for Random Forest\")\n",
    "        else:\n",
    "            print(f\"    → Adequate (500+ recommended, but {sample_count} is workable)\")\n",
    "    else:\n",
    "        print(f\"✗ CHECK 2 FAILED: Insufficient samples ({sample_count})\")\n",
    "        print(f\"    Minimum: 100 samples required\")\n",
    "        print(f\"    Recommended: 500+ samples\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CHECK 3: BINARY TARGET VARIABLE\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if validation_results['has_target_variable']:\n",
    "        target_classes = final_df[target_column].dropna().nunique()\n",
    "        validation_results['target_is_binary'] = (target_classes == 2)\n",
    "        \n",
    "        if validation_results['target_is_binary']:\n",
    "            print(f\"✓ CHECK 3 PASSED: Binary classification ({target_classes} classes)\")\n",
    "        else:\n",
    "            print(f\"✗ CHECK 3 FAILED: Expected 2 classes, found {target_classes}\")\n",
    "            print(f\"    Classes: {final_df[target_column].unique()}\")\n",
    "    else:\n",
    "        validation_results['target_is_binary'] = False\n",
    "        print(f\"⊘ CHECK 3 SKIPPED: No target variable to validate\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CHECK 4: TARGET CLASS BALANCE\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if validation_results['has_target_variable'] and validation_results['target_is_binary']:\n",
    "        target_value_counts = final_df[target_column].value_counts()\n",
    "        minority_class_pct = (target_value_counts.min() / target_value_counts.sum() * 100)\n",
    "        \n",
    "        validation_results['balanced_target'] = (minority_class_pct >= 20)\n",
    "        \n",
    "        print(f\"\\n  Target Distribution:\")\n",
    "        for class_val, count in target_value_counts.items():\n",
    "            pct = (count / target_value_counts.sum() * 100)\n",
    "            print(f\"    • {class_val}: {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        if validation_results['balanced_target']:\n",
    "            print(f\"✓ CHECK 4 PASSED: Reasonably balanced (minority: {minority_class_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"⚠ CHECK 4 WARNING: Imbalanced (minority: {minority_class_pct:.1f}%)\")\n",
    "            print(f\"    Recommendation: Use class_weight='balanced' in Random Forest\")\n",
    "            print(f\"    Note: This is not a critical failure\")\n",
    "    else:\n",
    "        validation_results['balanced_target'] = False\n",
    "        print(f\"⊘ CHECK 4 SKIPPED: No valid target variable\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CHECK 5: NO DUPLICATE ROWS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    duplicates = final_df.duplicated(subset=['game_id', 'play_id']).sum()\n",
    "    validation_results['no_duplicate_rows'] = (duplicates == 0)\n",
    "    \n",
    "    if validation_results['no_duplicate_rows']:\n",
    "        print(f\"✓ CHECK 5 PASSED: No duplicate play records\")\n",
    "    else:\n",
    "        print(f\"✗ CHECK 5 FAILED: Found {duplicates} duplicate plays\")\n",
    "        print(f\"    Action: Remove duplicates before training\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # CHECK 6: NUMERIC FEATURES VALID\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    numeric_cols = final_df.select_dtypes(include=[np.number]).columns\n",
    "    invalid_features = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # Skip identifier columns\n",
    "        if col in ['game_id', 'play_id', 'nfl_id', 'frame_id']:\n",
    "            continue\n",
    "        \n",
    "        # Check for infinite values\n",
    "        if np.isinf(final_df[col]).any():\n",
    "            invalid_features.append((col, 'infinite values'))\n",
    "        \n",
    "        # Check for excessive missing values\n",
    "        missing_pct = (final_df[col].isna().sum() / len(final_df) * 100)\n",
    "        if missing_pct > 50:\n",
    "            invalid_features.append((col, f'{missing_pct:.1f}% missing'))\n",
    "    \n",
    "    validation_results['numeric_features_valid'] = (len(invalid_features) == 0)\n",
    "    \n",
    "    if validation_results['numeric_features_valid']:\n",
    "        print(f\"✓ CHECK 6 PASSED: Numeric features valid ({len(numeric_cols)} features)\")\n",
    "    else:\n",
    "        print(f\"✗ CHECK 6 FAILED: Issues with {len(invalid_features)} features\")\n",
    "        for feature, issue in invalid_features[:5]:  # Show first 5\n",
    "            print(f\"    • {feature}: {issue}\")\n",
    "        if len(invalid_features) > 5:\n",
    "            print(f\"    ... and {len(invalid_features) - 5} more\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # OVERALL ML READINESS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Required checks (all must pass)\n",
    "    required_checks = [\n",
    "        'has_target_variable',\n",
    "        'sufficient_samples',\n",
    "        'target_is_binary',\n",
    "        'no_duplicate_rows',\n",
    "        'numeric_features_valid'\n",
    "    ]\n",
    "    \n",
    "    validation_results['ready_for_ml'] = all(\n",
    "        validation_results.get(check, False) for check in required_checks\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    if validation_results['ready_for_ml']:\n",
    "        print(\"✓✓✓ DATASET READY FOR ML PIPELINE ✓✓✓\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nRecommended Next Steps:\")\n",
    "        print(\"  1. Data Cleaning & Preprocessing\")\n",
    "        print(\"     - Handle outliers (IQR method)\")\n",
    "        print(\"     - Impute missing values (if any)\")\n",
    "        print(\"     - Encode categorical features\")\n",
    "        print(\"  2. Exploratory Data Analysis (EDA)\")\n",
    "        print(\"     - Feature distributions\")\n",
    "        print(\"     - Correlation analysis\")\n",
    "        print(\"     - Feature-target relationships\")\n",
    "        print(\"  3. Feature Selection\")\n",
    "        print(\"     - Remove highly correlated features\")\n",
    "        print(\"     - Feature importance analysis\")\n",
    "        print(\"  4. Model Training\")\n",
    "        print(\"     - Train/Test split (80/20)\")\n",
    "        print(\"     - Random Forest Classifier\")\n",
    "        print(\"     - Cross-validation\")\n",
    "        print(\"  5. Model Tuning & Evaluation\")\n",
    "        print(\"     - GridSearchCV for hyperparameters\")\n",
    "        print(\"     - Confusion matrix, ROC curve\")\n",
    "        print(\"     - Feature importance\")\n",
    "    else:\n",
    "        print(\"⚠⚠⚠ DATASET NEEDS ATTENTION BEFORE ML ⚠⚠⚠\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nFailed Checks (must be resolved):\")\n",
    "        for check, passed in validation_results.items():\n",
    "            if not passed and check != 'ready_for_ml' and check != 'balanced_target':\n",
    "                print(f\"  ✗ {check}\")\n",
    "        \n",
    "        print(\"\\nAction Required:\")\n",
    "        print(\"  1. Address failed checks above\")\n",
    "        print(\"  2. Re-run validation\")\n",
    "        print(\"  3. Proceed only after all checks pass\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return validation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca61c10",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82de0a",
   "metadata": {},
   "source": [
    "### Frame Level - QB Seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbc37976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: CALCULATING FRAME-LEVEL MINIMUM SEPARATIONS FOR QUARTERBACKS\n",
      "======================================================================\n",
      "Processing all frames using vectorized distance calculations...\n",
      "✓ Frame-level separation calculations complete\n",
      "✓ Returned 396765 Quarterback records with separation data\n",
      "✓ Columns: ['game_id', 'play_id', 'nfl_id', 'frame_id', 'player_name', 'player_role', 'player_position', 'qb_min_separation']\n"
     ]
    }
   ],
   "source": [
    "# Calculate frame-level minimum separation for Quarterbacks\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STEP 5: CALCULATING FRAME-LEVEL MINIMUM SEPARATIONS FOR QUARTERBACKS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Processing all frames using vectorized distance calculations...\")\n",
    "qb_separation_frame_level = calculate_frame_level_separation_qb(input_data)\n",
    "print(\"✓ Frame-level separation calculations complete\")\n",
    "print(f\"✓ Returned {len(qb_separation_frame_level)} Quarterback records with separation data\")\n",
    "print(f\"✓ Columns: {list(qb_separation_frame_level.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d5961",
   "metadata": {},
   "source": [
    "### Frame Level - QB Pressure Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09278d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/6] Calculating QB pressure velocity...\n",
      "    ✓ Added columns: separation_velocity, pressure_acceleration\n",
      "    ✓ Data shape: (396765, 10)\n",
      "✓ Columns: ['game_id', 'play_id', 'nfl_id', 'frame_id', 'player_name', 'player_role', 'player_position', 'qb_min_separation', 'separation_velocity', 'pressure_acceleration']\n"
     ]
    }
   ],
   "source": [
    "# Calculate Frame level QB pressure velocity\n",
    "print(\"\\n[1/6] Calculating QB pressure velocity...\")\n",
    "qb_velocity_frame_level = calculate_frame_level_pressure_velocity_qb(qb_separation_frame_level)\n",
    "print(f\"    ✓ Added columns: separation_velocity, pressure_acceleration\")\n",
    "print(f\"    ✓ Data shape: {qb_velocity_frame_level.shape}\")\n",
    "print(f\"✓ Columns: {list(qb_velocity_frame_level.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3921f",
   "metadata": {},
   "source": [
    "### Frame Level - QB Defender Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1e1536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CALCULATING DEFENDER CONVERGENCE\n",
      "======================================================================\n",
      "Pressure Zones: {'immediate': 3.0, 'closing': 5.0, 'potential': 7.0}\n",
      "----------------------------------------------------------------------\n",
      "Processing frames...\n",
      "  Processed 1,000 / 396,914 frames...\n",
      "  Processed 2,000 / 396,914 frames...\n",
      "  Processed 3,000 / 396,914 frames...\n",
      "  Processed 4,000 / 396,914 frames...\n",
      "  Processed 5,000 / 396,914 frames...\n",
      "  Processed 6,000 / 396,914 frames...\n",
      "  Processed 7,000 / 396,914 frames...\n",
      "  Processed 8,000 / 396,914 frames...\n",
      "  Processed 9,000 / 396,914 frames...\n",
      "  Processed 10,000 / 396,914 frames...\n",
      "  Processed 11,000 / 396,914 frames...\n",
      "  Processed 12,000 / 396,914 frames...\n",
      "  Processed 13,000 / 396,914 frames...\n",
      "  Processed 14,000 / 396,914 frames...\n",
      "  Processed 15,000 / 396,914 frames...\n",
      "  Processed 16,000 / 396,914 frames...\n",
      "  Processed 17,000 / 396,914 frames...\n",
      "  Processed 18,000 / 396,914 frames...\n",
      "  Processed 19,000 / 396,914 frames...\n",
      "  Processed 20,000 / 396,914 frames...\n",
      "  Processed 21,000 / 396,914 frames...\n",
      "  Processed 22,000 / 396,914 frames...\n",
      "  Processed 23,000 / 396,914 frames...\n",
      "  Processed 24,000 / 396,914 frames...\n",
      "  Processed 25,000 / 396,914 frames...\n",
      "  Processed 26,000 / 396,914 frames...\n",
      "  Processed 27,000 / 396,914 frames...\n",
      "  Processed 28,000 / 396,914 frames...\n",
      "  Processed 29,000 / 396,914 frames...\n",
      "  Processed 30,000 / 396,914 frames...\n",
      "  Processed 31,000 / 396,914 frames...\n",
      "  Processed 32,000 / 396,914 frames...\n",
      "  Processed 33,000 / 396,914 frames...\n",
      "  Processed 34,000 / 396,914 frames...\n",
      "  Processed 35,000 / 396,914 frames...\n",
      "  Processed 36,000 / 396,914 frames...\n",
      "  Processed 37,000 / 396,914 frames...\n",
      "  Processed 38,000 / 396,914 frames...\n",
      "  Processed 39,000 / 396,914 frames...\n",
      "  Processed 40,000 / 396,914 frames...\n",
      "  Processed 41,000 / 396,914 frames...\n",
      "  Processed 42,000 / 396,914 frames...\n",
      "  Processed 43,000 / 396,914 frames...\n",
      "  Processed 44,000 / 396,914 frames...\n",
      "  Processed 45,000 / 396,914 frames...\n",
      "  Processed 46,000 / 396,914 frames...\n",
      "  Processed 47,000 / 396,914 frames...\n",
      "  Processed 48,000 / 396,914 frames...\n",
      "  Processed 49,000 / 396,914 frames...\n",
      "  Processed 50,000 / 396,914 frames...\n",
      "  Processed 51,000 / 396,914 frames...\n",
      "  Processed 52,000 / 396,914 frames...\n",
      "  Processed 53,000 / 396,914 frames...\n",
      "  Processed 54,000 / 396,914 frames...\n",
      "  Processed 55,000 / 396,914 frames...\n",
      "  Processed 56,000 / 396,914 frames...\n",
      "  Processed 57,000 / 396,914 frames...\n",
      "  Processed 58,000 / 396,914 frames...\n",
      "  Processed 59,000 / 396,914 frames...\n",
      "  Processed 60,000 / 396,914 frames...\n",
      "  Processed 61,000 / 396,914 frames...\n",
      "  Processed 62,000 / 396,914 frames...\n",
      "  Processed 63,000 / 396,914 frames...\n",
      "  Processed 64,000 / 396,914 frames...\n",
      "  Processed 65,000 / 396,914 frames...\n",
      "  Processed 66,000 / 396,914 frames...\n",
      "  Processed 67,000 / 396,914 frames...\n",
      "  Processed 68,000 / 396,914 frames...\n",
      "  Processed 69,000 / 396,914 frames...\n",
      "  Processed 70,000 / 396,914 frames...\n",
      "  Processed 71,000 / 396,914 frames...\n",
      "  Processed 72,000 / 396,914 frames...\n",
      "  Processed 73,000 / 396,914 frames...\n",
      "  Processed 74,000 / 396,914 frames...\n",
      "  Processed 75,000 / 396,914 frames...\n",
      "  Processed 76,000 / 396,914 frames...\n",
      "  Processed 77,000 / 396,914 frames...\n",
      "  Processed 78,000 / 396,914 frames...\n",
      "  Processed 79,000 / 396,914 frames...\n",
      "  Processed 80,000 / 396,914 frames...\n",
      "  Processed 81,000 / 396,914 frames...\n",
      "  Processed 82,000 / 396,914 frames...\n",
      "  Processed 83,000 / 396,914 frames...\n",
      "  Processed 84,000 / 396,914 frames...\n",
      "  Processed 85,000 / 396,914 frames...\n",
      "  Processed 86,000 / 396,914 frames...\n",
      "  Processed 87,000 / 396,914 frames...\n",
      "  Processed 88,000 / 396,914 frames...\n",
      "  Processed 89,000 / 396,914 frames...\n",
      "  Processed 90,000 / 396,914 frames...\n",
      "  Processed 91,000 / 396,914 frames...\n",
      "  Processed 92,000 / 396,914 frames...\n",
      "  Processed 93,000 / 396,914 frames...\n",
      "  Processed 94,000 / 396,914 frames...\n",
      "  Processed 95,000 / 396,914 frames...\n",
      "  Processed 96,000 / 396,914 frames...\n",
      "  Processed 97,000 / 396,914 frames...\n",
      "  Processed 98,000 / 396,914 frames...\n",
      "  Processed 99,000 / 396,914 frames...\n",
      "  Processed 100,000 / 396,914 frames...\n",
      "  Processed 101,000 / 396,914 frames...\n",
      "  Processed 102,000 / 396,914 frames...\n",
      "  Processed 103,000 / 396,914 frames...\n",
      "  Processed 104,000 / 396,914 frames...\n",
      "  Processed 105,000 / 396,914 frames...\n",
      "  Processed 106,000 / 396,914 frames...\n",
      "  Processed 107,000 / 396,914 frames...\n",
      "  Processed 108,000 / 396,914 frames...\n",
      "  Processed 109,000 / 396,914 frames...\n",
      "  Processed 110,000 / 396,914 frames...\n",
      "  Processed 111,000 / 396,914 frames...\n",
      "  Processed 112,000 / 396,914 frames...\n",
      "  Processed 113,000 / 396,914 frames...\n",
      "  Processed 114,000 / 396,914 frames...\n",
      "  Processed 115,000 / 396,914 frames...\n",
      "  Processed 116,000 / 396,914 frames...\n",
      "  Processed 117,000 / 396,914 frames...\n",
      "  Processed 118,000 / 396,914 frames...\n",
      "  Processed 119,000 / 396,914 frames...\n",
      "  Processed 120,000 / 396,914 frames...\n",
      "  Processed 121,000 / 396,914 frames...\n",
      "  Processed 122,000 / 396,914 frames...\n",
      "  Processed 123,000 / 396,914 frames...\n",
      "  Processed 124,000 / 396,914 frames...\n",
      "  Processed 125,000 / 396,914 frames...\n",
      "  Processed 126,000 / 396,914 frames...\n",
      "  Processed 127,000 / 396,914 frames...\n",
      "  Processed 128,000 / 396,914 frames...\n",
      "  Processed 129,000 / 396,914 frames...\n",
      "  Processed 130,000 / 396,914 frames...\n",
      "  Processed 131,000 / 396,914 frames...\n",
      "  Processed 132,000 / 396,914 frames...\n",
      "  Processed 133,000 / 396,914 frames...\n",
      "  Processed 134,000 / 396,914 frames...\n",
      "  Processed 135,000 / 396,914 frames...\n",
      "  Processed 136,000 / 396,914 frames...\n",
      "  Processed 137,000 / 396,914 frames...\n",
      "  Processed 138,000 / 396,914 frames...\n",
      "  Processed 139,000 / 396,914 frames...\n",
      "  Processed 140,000 / 396,914 frames...\n",
      "  Processed 141,000 / 396,914 frames...\n",
      "  Processed 142,000 / 396,914 frames...\n",
      "  Processed 143,000 / 396,914 frames...\n",
      "  Processed 144,000 / 396,914 frames...\n",
      "  Processed 145,000 / 396,914 frames...\n",
      "  Processed 146,000 / 396,914 frames...\n",
      "  Processed 147,000 / 396,914 frames...\n",
      "  Processed 148,000 / 396,914 frames...\n",
      "  Processed 149,000 / 396,914 frames...\n",
      "  Processed 150,000 / 396,914 frames...\n",
      "  Processed 151,000 / 396,914 frames...\n",
      "  Processed 152,000 / 396,914 frames...\n",
      "  Processed 153,000 / 396,914 frames...\n",
      "  Processed 154,000 / 396,914 frames...\n",
      "  Processed 155,000 / 396,914 frames...\n",
      "  Processed 156,000 / 396,914 frames...\n",
      "  Processed 157,000 / 396,914 frames...\n",
      "  Processed 158,000 / 396,914 frames...\n",
      "  Processed 159,000 / 396,914 frames...\n",
      "  Processed 160,000 / 396,914 frames...\n",
      "  Processed 161,000 / 396,914 frames...\n",
      "  Processed 162,000 / 396,914 frames...\n",
      "  Processed 163,000 / 396,914 frames...\n",
      "  Processed 164,000 / 396,914 frames...\n",
      "  Processed 165,000 / 396,914 frames...\n",
      "  Processed 166,000 / 396,914 frames...\n",
      "  Processed 167,000 / 396,914 frames...\n",
      "  Processed 168,000 / 396,914 frames...\n",
      "  Processed 169,000 / 396,914 frames...\n",
      "  Processed 170,000 / 396,914 frames...\n",
      "  Processed 171,000 / 396,914 frames...\n",
      "  Processed 172,000 / 396,914 frames...\n",
      "  Processed 173,000 / 396,914 frames...\n",
      "  Processed 174,000 / 396,914 frames...\n",
      "  Processed 175,000 / 396,914 frames...\n",
      "  Processed 176,000 / 396,914 frames...\n",
      "  Processed 177,000 / 396,914 frames...\n",
      "  Processed 178,000 / 396,914 frames...\n",
      "  Processed 179,000 / 396,914 frames...\n",
      "  Processed 180,000 / 396,914 frames...\n",
      "  Processed 181,000 / 396,914 frames...\n",
      "  Processed 182,000 / 396,914 frames...\n",
      "  Processed 183,000 / 396,914 frames...\n",
      "  Processed 184,000 / 396,914 frames...\n",
      "  Processed 185,000 / 396,914 frames...\n",
      "  Processed 186,000 / 396,914 frames...\n",
      "  Processed 187,000 / 396,914 frames...\n",
      "  Processed 188,000 / 396,914 frames...\n",
      "  Processed 189,000 / 396,914 frames...\n",
      "  Processed 190,000 / 396,914 frames...\n",
      "  Processed 191,000 / 396,914 frames...\n",
      "  Processed 192,000 / 396,914 frames...\n",
      "  Processed 193,000 / 396,914 frames...\n",
      "  Processed 194,000 / 396,914 frames...\n",
      "  Processed 195,000 / 396,914 frames...\n",
      "  Processed 196,000 / 396,914 frames...\n",
      "  Processed 197,000 / 396,914 frames...\n",
      "  Processed 198,000 / 396,914 frames...\n",
      "  Processed 199,000 / 396,914 frames...\n",
      "  Processed 200,000 / 396,914 frames...\n",
      "  Processed 201,000 / 396,914 frames...\n",
      "  Processed 202,000 / 396,914 frames...\n",
      "  Processed 203,000 / 396,914 frames...\n",
      "  Processed 204,000 / 396,914 frames...\n",
      "  Processed 205,000 / 396,914 frames...\n",
      "  Processed 206,000 / 396,914 frames...\n",
      "  Processed 207,000 / 396,914 frames...\n",
      "  Processed 208,000 / 396,914 frames...\n",
      "  Processed 209,000 / 396,914 frames...\n",
      "  Processed 210,000 / 396,914 frames...\n",
      "  Processed 211,000 / 396,914 frames...\n",
      "  Processed 212,000 / 396,914 frames...\n",
      "  Processed 213,000 / 396,914 frames...\n",
      "  Processed 214,000 / 396,914 frames...\n",
      "  Processed 215,000 / 396,914 frames...\n",
      "  Processed 216,000 / 396,914 frames...\n",
      "  Processed 217,000 / 396,914 frames...\n",
      "  Processed 218,000 / 396,914 frames...\n",
      "  Processed 219,000 / 396,914 frames...\n",
      "  Processed 220,000 / 396,914 frames...\n",
      "  Processed 221,000 / 396,914 frames...\n",
      "  Processed 222,000 / 396,914 frames...\n",
      "  Processed 223,000 / 396,914 frames...\n",
      "  Processed 224,000 / 396,914 frames...\n",
      "  Processed 225,000 / 396,914 frames...\n",
      "  Processed 226,000 / 396,914 frames...\n",
      "  Processed 227,000 / 396,914 frames...\n",
      "  Processed 228,000 / 396,914 frames...\n",
      "  Processed 229,000 / 396,914 frames...\n",
      "  Processed 230,000 / 396,914 frames...\n",
      "  Processed 231,000 / 396,914 frames...\n",
      "  Processed 232,000 / 396,914 frames...\n",
      "  Processed 233,000 / 396,914 frames...\n",
      "  Processed 234,000 / 396,914 frames...\n",
      "  Processed 235,000 / 396,914 frames...\n",
      "  Processed 236,000 / 396,914 frames...\n",
      "  Processed 237,000 / 396,914 frames...\n",
      "  Processed 238,000 / 396,914 frames...\n",
      "  Processed 239,000 / 396,914 frames...\n",
      "  Processed 240,000 / 396,914 frames...\n",
      "  Processed 241,000 / 396,914 frames...\n",
      "  Processed 242,000 / 396,914 frames...\n",
      "  Processed 243,000 / 396,914 frames...\n",
      "  Processed 244,000 / 396,914 frames...\n",
      "  Processed 245,000 / 396,914 frames...\n",
      "  Processed 246,000 / 396,914 frames...\n",
      "  Processed 247,000 / 396,914 frames...\n",
      "  Processed 248,000 / 396,914 frames...\n",
      "  Processed 249,000 / 396,914 frames...\n",
      "  Processed 250,000 / 396,914 frames...\n",
      "  Processed 251,000 / 396,914 frames...\n",
      "  Processed 252,000 / 396,914 frames...\n",
      "  Processed 253,000 / 396,914 frames...\n",
      "  Processed 254,000 / 396,914 frames...\n",
      "  Processed 255,000 / 396,914 frames...\n",
      "  Processed 256,000 / 396,914 frames...\n",
      "  Processed 257,000 / 396,914 frames...\n",
      "  Processed 258,000 / 396,914 frames...\n",
      "  Processed 259,000 / 396,914 frames...\n",
      "  Processed 260,000 / 396,914 frames...\n",
      "  Processed 261,000 / 396,914 frames...\n",
      "  Processed 262,000 / 396,914 frames...\n",
      "  Processed 263,000 / 396,914 frames...\n",
      "  Processed 264,000 / 396,914 frames...\n",
      "  Processed 265,000 / 396,914 frames...\n",
      "  Processed 266,000 / 396,914 frames...\n",
      "  Processed 267,000 / 396,914 frames...\n",
      "  Processed 268,000 / 396,914 frames...\n",
      "  Processed 269,000 / 396,914 frames...\n",
      "  Processed 270,000 / 396,914 frames...\n",
      "  Processed 271,000 / 396,914 frames...\n",
      "  Processed 272,000 / 396,914 frames...\n",
      "  Processed 273,000 / 396,914 frames...\n",
      "  Processed 274,000 / 396,914 frames...\n",
      "  Processed 275,000 / 396,914 frames...\n",
      "  Processed 276,000 / 396,914 frames...\n",
      "  Processed 277,000 / 396,914 frames...\n",
      "  Processed 278,000 / 396,914 frames...\n",
      "  Processed 279,000 / 396,914 frames...\n",
      "  Processed 280,000 / 396,914 frames...\n",
      "  Processed 281,000 / 396,914 frames...\n",
      "  Processed 282,000 / 396,914 frames...\n",
      "  Processed 283,000 / 396,914 frames...\n",
      "  Processed 284,000 / 396,914 frames...\n",
      "  Processed 285,000 / 396,914 frames...\n",
      "  Processed 286,000 / 396,914 frames...\n",
      "  Processed 287,000 / 396,914 frames...\n",
      "  Processed 288,000 / 396,914 frames...\n",
      "  Processed 289,000 / 396,914 frames...\n",
      "  Processed 290,000 / 396,914 frames...\n",
      "  Processed 291,000 / 396,914 frames...\n",
      "  Processed 292,000 / 396,914 frames...\n",
      "  Processed 293,000 / 396,914 frames...\n",
      "  Processed 294,000 / 396,914 frames...\n",
      "  Processed 295,000 / 396,914 frames...\n",
      "  Processed 296,000 / 396,914 frames...\n",
      "  Processed 297,000 / 396,914 frames...\n",
      "  Processed 298,000 / 396,914 frames...\n",
      "  Processed 299,000 / 396,914 frames...\n",
      "  Processed 300,000 / 396,914 frames...\n",
      "  Processed 301,000 / 396,914 frames...\n",
      "  Processed 302,000 / 396,914 frames...\n",
      "  Processed 303,000 / 396,914 frames...\n",
      "  Processed 304,000 / 396,914 frames...\n",
      "  Processed 305,000 / 396,914 frames...\n",
      "  Processed 306,000 / 396,914 frames...\n",
      "  Processed 307,000 / 396,914 frames...\n",
      "  Processed 308,000 / 396,914 frames...\n",
      "  Processed 309,000 / 396,914 frames...\n",
      "  Processed 310,000 / 396,914 frames...\n",
      "  Processed 311,000 / 396,914 frames...\n",
      "  Processed 312,000 / 396,914 frames...\n",
      "  Processed 313,000 / 396,914 frames...\n",
      "  Processed 314,000 / 396,914 frames...\n",
      "  Processed 315,000 / 396,914 frames...\n",
      "  Processed 316,000 / 396,914 frames...\n",
      "  Processed 317,000 / 396,914 frames...\n",
      "  Processed 318,000 / 396,914 frames...\n",
      "  Processed 319,000 / 396,914 frames...\n",
      "  Processed 320,000 / 396,914 frames...\n",
      "  Processed 321,000 / 396,914 frames...\n",
      "  Processed 322,000 / 396,914 frames...\n",
      "  Processed 323,000 / 396,914 frames...\n",
      "  Processed 324,000 / 396,914 frames...\n",
      "  Processed 325,000 / 396,914 frames...\n",
      "  Processed 326,000 / 396,914 frames...\n",
      "  Processed 327,000 / 396,914 frames...\n",
      "  Processed 328,000 / 396,914 frames...\n",
      "  Processed 329,000 / 396,914 frames...\n",
      "  Processed 330,000 / 396,914 frames...\n",
      "  Processed 331,000 / 396,914 frames...\n",
      "  Processed 332,000 / 396,914 frames...\n",
      "  Processed 333,000 / 396,914 frames...\n",
      "  Processed 334,000 / 396,914 frames...\n",
      "  Processed 335,000 / 396,914 frames...\n",
      "  Processed 336,000 / 396,914 frames...\n",
      "  Processed 337,000 / 396,914 frames...\n",
      "  Processed 338,000 / 396,914 frames...\n",
      "  Processed 339,000 / 396,914 frames...\n",
      "  Processed 340,000 / 396,914 frames...\n",
      "  Processed 341,000 / 396,914 frames...\n",
      "  Processed 342,000 / 396,914 frames...\n",
      "  Processed 343,000 / 396,914 frames...\n",
      "  Processed 344,000 / 396,914 frames...\n",
      "  Processed 345,000 / 396,914 frames...\n",
      "  Processed 346,000 / 396,914 frames...\n",
      "  Processed 347,000 / 396,914 frames...\n",
      "  Processed 348,000 / 396,914 frames...\n",
      "  Processed 349,000 / 396,914 frames...\n",
      "  Processed 350,000 / 396,914 frames...\n",
      "  Processed 351,000 / 396,914 frames...\n",
      "  Processed 352,000 / 396,914 frames...\n",
      "  Processed 353,000 / 396,914 frames...\n",
      "  Processed 354,000 / 396,914 frames...\n",
      "  Processed 355,000 / 396,914 frames...\n",
      "  Processed 356,000 / 396,914 frames...\n",
      "  Processed 357,000 / 396,914 frames...\n",
      "  Processed 358,000 / 396,914 frames...\n",
      "  Processed 359,000 / 396,914 frames...\n",
      "  Processed 360,000 / 396,914 frames...\n",
      "  Processed 361,000 / 396,914 frames...\n",
      "  Processed 362,000 / 396,914 frames...\n",
      "  Processed 363,000 / 396,914 frames...\n",
      "  Processed 364,000 / 396,914 frames...\n",
      "  Processed 365,000 / 396,914 frames...\n",
      "  Processed 366,000 / 396,914 frames...\n",
      "  Processed 367,000 / 396,914 frames...\n",
      "  Processed 368,000 / 396,914 frames...\n",
      "  Processed 369,000 / 396,914 frames...\n",
      "  Processed 370,000 / 396,914 frames...\n",
      "  Processed 371,000 / 396,914 frames...\n",
      "  Processed 372,000 / 396,914 frames...\n",
      "  Processed 373,000 / 396,914 frames...\n",
      "  Processed 374,000 / 396,914 frames...\n",
      "  Processed 375,000 / 396,914 frames...\n",
      "  Processed 376,000 / 396,914 frames...\n",
      "  Processed 377,000 / 396,914 frames...\n",
      "  Processed 378,000 / 396,914 frames...\n",
      "  Processed 379,000 / 396,914 frames...\n",
      "  Processed 380,000 / 396,914 frames...\n",
      "  Processed 381,000 / 396,914 frames...\n",
      "  Processed 382,000 / 396,914 frames...\n",
      "  Processed 383,000 / 396,914 frames...\n",
      "  Processed 384,000 / 396,914 frames...\n",
      "  Processed 385,000 / 396,914 frames...\n",
      "  Processed 386,000 / 396,914 frames...\n",
      "  Processed 387,000 / 396,914 frames...\n",
      "  Processed 388,000 / 396,914 frames...\n",
      "  Processed 389,000 / 396,914 frames...\n",
      "  Processed 390,000 / 396,914 frames...\n",
      "  Processed 391,000 / 396,914 frames...\n",
      "  Processed 392,000 / 396,914 frames...\n",
      "  Processed 393,000 / 396,914 frames...\n",
      "  Processed 394,000 / 396,914 frames...\n",
      "  Processed 395,000 / 396,914 frames...\n",
      "  Processed 396,000 / 396,914 frames...\n",
      "\n",
      "======================================================================\n",
      "CONVERGENCE CALCULATION COMPLETE\n",
      "======================================================================\n",
      "Total frames processed: 396,914\n",
      "Frames with QB & defenders: 396,765\n",
      "Convergence records created: 396,765\n",
      "\n",
      "Convergence Category Distribution:\n",
      "  • Critical    : 2,292 frames (  0.6%)\n",
      "  • High        : 9,953 frames (  2.5%)\n",
      "  • Moderate    : 22,923 frames (  5.8%)\n",
      "  • None        : 361,597 frames ( 91.1%)\n",
      "\n",
      "Defender Zone Statistics:\n",
      "  • Avg defenders within 3yds: 0.00\n",
      "  • Avg defenders within 5yds: 0.00\n",
      "  • Avg defenders within 7yds: 0.12\n",
      "\n",
      "Closest Defender Statistics:\n",
      "  • Min distance: 2.09 yards\n",
      "  • Avg distance: 11.22 yards\n",
      "  • Max distance: 39.48 yards\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "frame_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "defenders_closing_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "convergence_category",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8c27c77f-be33-4d58-a07b-92ca44109f80",
       "rows": [
        [
         "0",
         "1",
         "0",
         "None"
        ],
        [
         "1",
         "2",
         "0",
         "None"
        ],
        [
         "2",
         "3",
         "0",
         "None"
        ],
        [
         "3",
         "4",
         "0",
         "None"
        ],
        [
         "4",
         "5",
         "0",
         "None"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame_id</th>\n",
       "      <th>defenders_closing_zone</th>\n",
       "      <th>convergence_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame_id  defenders_closing_zone convergence_category\n",
       "0         1                       0                 None\n",
       "1         2                       0                 None\n",
       "2         3                       0                 None\n",
       "3         4                       0                 None\n",
       "4         5                       0                 None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames with high convergence: 9953\n",
      "Frames with QB separation below 3 yards: 61\n"
     ]
    }
   ],
   "source": [
    "# Calculate Frame level QB defender Convergence\n",
    "convergence_frame_level = calculate_frame_level_defender_convergence(input_data)\n",
    "display(convergence_frame_level[['frame_id', 'defenders_closing_zone', 'convergence_category']].head())\n",
    "high_conv = convergence_frame_level[convergence_frame_level['convergence_category'] == 'High']\n",
    "print(f\"Frames with high convergence: {len(high_conv)}\")\n",
    "\n",
    "# List row with minimum separation is below 3 yards\n",
    "min_sep_below_3yds = qb_velocity_frame_level[qb_velocity_frame_level['qb_min_separation'] < 3.0]\n",
    "# display(min_sep_below_3yds[['game_id', 'play_id', 'frame_id', 'qb_min_separation', 'defenders_immediate_zone']])\n",
    "print(f\"Frames with QB separation below 3 yards: {len(min_sep_below_3yds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d150ef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "game_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "play_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "frame_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "nfl_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "player_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "player_role",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "player_position",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "defenders_immediate_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "defenders_closing_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "defenders_potential_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "total_converging_defenders",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "convergence_category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "closest_defender_distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_defenders_on_field",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "d6c2ddb7-3029-476c-9424-41bab69f3751",
       "rows": [
        [
         "21205",
         "2023091013",
         "1737",
         "49",
         "47789",
         "Daniel Jones",
         "Passer",
         "QB",
         "1",
         "0",
         "0",
         "1",
         "Moderate",
         "2.7262428358457007",
         "6"
        ],
        [
         "21206",
         "2023091013",
         "1737",
         "50",
         "47789",
         "Daniel Jones",
         "Passer",
         "QB",
         "1",
         "0",
         "0",
         "1",
         "Moderate",
         "2.3940969069776603",
         "6"
        ],
        [
         "21207",
         "2023091013",
         "1737",
         "51",
         "47789",
         "Daniel Jones",
         "Passer",
         "QB",
         "1",
         "0",
         "0",
         "1",
         "Moderate",
         "2.087223035518721",
         "6"
        ],
        [
         "44216",
         "2023091800",
         "1854",
         "59",
         "55865",
         "Bryce Young",
         "Passer",
         "QB",
         "1",
         "0",
         "0",
         "1",
         "Moderate",
         "2.748545069668685",
         "8"
        ],
        [
         "110235",
         "2023100810",
         "2049",
         "50",
         "44822",
         "Patrick Mahomes",
         "Passer",
         "QB",
         "1",
         "0",
         "0",
         "1",
         "Moderate",
         "2.8778464170278433",
         "7"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>play_id</th>\n",
       "      <th>frame_id</th>\n",
       "      <th>nfl_id</th>\n",
       "      <th>player_name</th>\n",
       "      <th>player_role</th>\n",
       "      <th>player_position</th>\n",
       "      <th>defenders_immediate_zone</th>\n",
       "      <th>defenders_closing_zone</th>\n",
       "      <th>defenders_potential_zone</th>\n",
       "      <th>total_converging_defenders</th>\n",
       "      <th>convergence_category</th>\n",
       "      <th>closest_defender_distance</th>\n",
       "      <th>total_defenders_on_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21205</th>\n",
       "      <td>2023091013</td>\n",
       "      <td>1737</td>\n",
       "      <td>49</td>\n",
       "      <td>47789</td>\n",
       "      <td>Daniel Jones</td>\n",
       "      <td>Passer</td>\n",
       "      <td>QB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.726243</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21206</th>\n",
       "      <td>2023091013</td>\n",
       "      <td>1737</td>\n",
       "      <td>50</td>\n",
       "      <td>47789</td>\n",
       "      <td>Daniel Jones</td>\n",
       "      <td>Passer</td>\n",
       "      <td>QB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.394097</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21207</th>\n",
       "      <td>2023091013</td>\n",
       "      <td>1737</td>\n",
       "      <td>51</td>\n",
       "      <td>47789</td>\n",
       "      <td>Daniel Jones</td>\n",
       "      <td>Passer</td>\n",
       "      <td>QB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.087223</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44216</th>\n",
       "      <td>2023091800</td>\n",
       "      <td>1854</td>\n",
       "      <td>59</td>\n",
       "      <td>55865</td>\n",
       "      <td>Bryce Young</td>\n",
       "      <td>Passer</td>\n",
       "      <td>QB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.748545</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110235</th>\n",
       "      <td>2023100810</td>\n",
       "      <td>2049</td>\n",
       "      <td>50</td>\n",
       "      <td>44822</td>\n",
       "      <td>Patrick Mahomes</td>\n",
       "      <td>Passer</td>\n",
       "      <td>QB</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>2.877846</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           game_id  play_id  frame_id  nfl_id      player_name player_role  \\\n",
       "21205   2023091013     1737        49   47789     Daniel Jones      Passer   \n",
       "21206   2023091013     1737        50   47789     Daniel Jones      Passer   \n",
       "21207   2023091013     1737        51   47789     Daniel Jones      Passer   \n",
       "44216   2023091800     1854        59   55865      Bryce Young      Passer   \n",
       "110235  2023100810     2049        50   44822  Patrick Mahomes      Passer   \n",
       "\n",
       "       player_position  defenders_immediate_zone  defenders_closing_zone  \\\n",
       "21205               QB                         1                       0   \n",
       "21206               QB                         1                       0   \n",
       "21207               QB                         1                       0   \n",
       "44216               QB                         1                       0   \n",
       "110235              QB                         1                       0   \n",
       "\n",
       "        defenders_potential_zone  total_converging_defenders  \\\n",
       "21205                          0                           1   \n",
       "21206                          0                           1   \n",
       "21207                          0                           1   \n",
       "44216                          0                           1   \n",
       "110235                         0                           1   \n",
       "\n",
       "       convergence_category  closest_defender_distance  \\\n",
       "21205              Moderate                   2.726243   \n",
       "21206              Moderate                   2.394097   \n",
       "21207              Moderate                   2.087223   \n",
       "44216              Moderate                   2.748545   \n",
       "110235             Moderate                   2.877846   \n",
       "\n",
       "        total_defenders_on_field  \n",
       "21205                          6  \n",
       "21206                          6  \n",
       "21207                          6  \n",
       "44216                          8  \n",
       "110235                         7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_sep_belo3yds = convergence_frame_level[convergence_frame_level['closest_defender_distance'] < 3.0]\n",
    "display(min_sep_belo3yds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81813ec1",
   "metadata": {},
   "source": [
    "### Frame Level - QB Merged Data Separation + Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f455e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Merging QB separation with convergence metrics...\n",
      "Input: qb_velocity_frame_level + convergence_frame_level\n",
      "Output: Merged pressure features (NO collapse indicators yet)\n",
      "======================================================================\n",
      "MERGING QB SEPARATION WITH CONVERGENCE (FRAME-LEVEL)\n",
      "======================================================================\n",
      "QB separation data: 396,765 frames\n",
      "Convergence data: 396,765 frames\n",
      "\n",
      "Purpose: Combine pressure metrics for collapse detection\n",
      "Note: Collapse indicators created in next step\n",
      "\n",
      "Merge Results:\n",
      "  • Total frames after merge: 396,765\n",
      "  • Frames with convergence data: 396,765\n",
      "  • Match rate: 100.0%\n",
      "  • Total columns: 16\n",
      "  ✓ Good match rate - data sources aligned\n",
      "======================================================================\n",
      "\n",
      "✓ Frame-level merge completed successfully\n",
      "✓ Output shape: Index(['game_id', 'play_id', 'nfl_id', 'frame_id', 'player_name',\n",
      "       'player_role', 'player_position', 'qb_min_separation',\n",
      "       'separation_velocity', 'pressure_acceleration',\n",
      "       'defenders_immediate_zone', 'defenders_closing_zone',\n",
      "       'defenders_potential_zone', 'total_converging_defenders',\n",
      "       'convergence_category', 'total_defenders_on_field'],\n",
      "      dtype='object') columns, 396,765 rows\n",
      "✓ Ready for collapse detection\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Merge Frame-Level QB Separation with Convergence\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[1/4] Merging QB separation with convergence metrics...\")\n",
    "print(\"Input: qb_velocity_frame_level + convergence_frame_level\")\n",
    "print(\"Output: Merged pressure features (NO collapse indicators yet)\")\n",
    "\n",
    "try:\n",
    "    qb_features_frame_level = merge_qb_separation_with_convergence(\n",
    "        qb_velocity_frame_level,    # QB separation + velocity (from earlier steps)\n",
    "        convergence_frame_level,    # Convergence metrics (from Step A.2)\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Frame-level merge completed successfully\")\n",
    "    print(f\"✓ Output shape: {qb_features_frame_level.columns} columns, {len(qb_features_frame_level):,} rows\")\n",
    "    print(f\"✓ Ready for collapse detection\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error merging frame-level data: {e}\")\n",
    "    print(f\"   Check that qb_velocity_frame_level and convergence_frame_level exist\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cdbf702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "game_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "play_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "frame_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "qb_min_separation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "defenders_immediate_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "defenders_closing_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "defenders_potential_zone",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "convergence_category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "total_converging_defenders",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a06d69d2-bcc3-49be-ae4f-c9bb5f700e75",
       "rows": [
        [
         "21205",
         "2023091013",
         "1737",
         "49",
         "2.73",
         "1",
         "0",
         "0",
         "Moderate",
         "1"
        ],
        [
         "21206",
         "2023091013",
         "1737",
         "50",
         "2.39",
         "1",
         "0",
         "0",
         "Moderate",
         "1"
        ],
        [
         "21207",
         "2023091013",
         "1737",
         "51",
         "2.09",
         "1",
         "0",
         "0",
         "Moderate",
         "1"
        ],
        [
         "44216",
         "2023091800",
         "1854",
         "59",
         "2.75",
         "1",
         "0",
         "0",
         "Moderate",
         "1"
        ],
        [
         "110235",
         "2023100810",
         "2049",
         "50",
         "2.88",
         "1",
         "0",
         "0",
         "Moderate",
         "1"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>play_id</th>\n",
       "      <th>frame_id</th>\n",
       "      <th>qb_min_separation</th>\n",
       "      <th>defenders_immediate_zone</th>\n",
       "      <th>defenders_closing_zone</th>\n",
       "      <th>defenders_potential_zone</th>\n",
       "      <th>convergence_category</th>\n",
       "      <th>total_converging_defenders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21205</th>\n",
       "      <td>2023091013</td>\n",
       "      <td>1737</td>\n",
       "      <td>49</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21206</th>\n",
       "      <td>2023091013</td>\n",
       "      <td>1737</td>\n",
       "      <td>50</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21207</th>\n",
       "      <td>2023091013</td>\n",
       "      <td>1737</td>\n",
       "      <td>51</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44216</th>\n",
       "      <td>2023091800</td>\n",
       "      <td>1854</td>\n",
       "      <td>59</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110235</th>\n",
       "      <td>2023100810</td>\n",
       "      <td>2049</td>\n",
       "      <td>50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           game_id  play_id  frame_id  qb_min_separation  \\\n",
       "21205   2023091013     1737        49               2.73   \n",
       "21206   2023091013     1737        50               2.39   \n",
       "21207   2023091013     1737        51               2.09   \n",
       "44216   2023091800     1854        59               2.75   \n",
       "110235  2023100810     2049        50               2.88   \n",
       "\n",
       "        defenders_immediate_zone  defenders_closing_zone  \\\n",
       "21205                          1                       0   \n",
       "21206                          1                       0   \n",
       "21207                          1                       0   \n",
       "44216                          1                       0   \n",
       "110235                         1                       0   \n",
       "\n",
       "        defenders_potential_zone convergence_category  \\\n",
       "21205                          0             Moderate   \n",
       "21206                          0             Moderate   \n",
       "21207                          0             Moderate   \n",
       "44216                          0             Moderate   \n",
       "110235                         0             Moderate   \n",
       "\n",
       "        total_converging_defenders  \n",
       "21205                            1  \n",
       "21206                            1  \n",
       "21207                            1  \n",
       "44216                            1  \n",
       "110235                           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = qb_features_frame_level[qb_features_frame_level['qb_min_separation'] < 3.0]\n",
    "# display only relevant columns\n",
    "display(test[['game_id', 'play_id', 'frame_id', 'qb_min_separation','defenders_immediate_zone','defenders_closing_zone', 'defenders_potential_zone',\n",
    "                'convergence_category', 'total_converging_defenders' ]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1d69e",
   "metadata": {},
   "source": [
    "### Frame Level - TR Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ca1c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " CALCULATING FRAME-LEVEL MINIMUM SEPARATIONS\n",
      "======================================================================\n",
      "Processing all frames using vectorized distance calculations...\n",
      "✓ Frame-level separation calculations complete\n",
      "✓ Returned 396879 Targeted Receiver records with separation data\n",
      "✓ Columns: ['game_id', 'play_id', 'nfl_id', 'frame_id', 'player_name', 'player_role', 'player_position', 'tr_min_separation', 'route_of_targeted_receiver', 'dir']\n"
     ]
    }
   ],
   "source": [
    "# Calculate frame-level minimum separation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\" CALCULATING FRAME-LEVEL MINIMUM SEPARATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Processing all frames using vectorized distance calculations...\")\n",
    "tr_separation_df = calculate_frame_level_separation_tr(input_data)\n",
    "print(\"✓ Frame-level separation calculations complete\")\n",
    "print(f\"✓ Returned {len(tr_separation_df)} Targeted Receiver records with separation data\")\n",
    "print(f\"✓ Columns: {list(tr_separation_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6468a",
   "metadata": {},
   "source": [
    "### Play Level - QB Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c36889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AGGREGATING QB FRAME-LEVEL FEATURES TO PLAY-LEVEL\n",
      "======================================================================\n",
      "Input: 396,765 frames\n",
      "✓ Aggregated to 14,105 rows\n",
      "✓ Features: 20 columns\n",
      "Output: 14,105 plays × 20 features\n",
      "Reduction: 28.1:1 (frames to plays)\n",
      "\n",
      "Feature Groups:\n",
      "  • Separation     :  3 features\n",
      "  • Velocity       :  4 features\n",
      "  • Convergence    :  7 features\n",
      "======================================================================\n",
      "Index(['game_id', 'play_id', 'nfl_id', 'qb_play_min_separation',\n",
      "       'qb_play_avg_separation', 'qb_play_var_separation',\n",
      "       'max_pressure_velocity', 'pressure_velocity_avg', 'pressure_volatility',\n",
      "       'max_pressure_acceleration', 'pressure_acceleration_avg',\n",
      "       'max_converging_defenders', 'avg_converging_defenders',\n",
      "       'std_converging_defenders', 'max_defenders_immediate_zone',\n",
      "       'max_defenders_closing_zone', 'avg_defenders_closing_zone',\n",
      "       'max_defenders_potential_zone', 'player_name', 'player_role'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Play Level aggregation (QB + convergence + collapse)\n",
    "complete_qb_play_features = aggregate_qb_features_to_play_level(\n",
    "    qb_features_frame_level  # ← Complete frame data\n",
    ")\n",
    "\n",
    "print(complete_qb_play_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a47b0",
   "metadata": {},
   "source": [
    "### Play Level - TR Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da84b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR Play-level aggregates shape: (14107, 12)\n",
      "Index(['game_id', 'play_id', 'nfl_id', 'tr_play_min_separation',\n",
      "       'tr_play_avg_separation', 'tr_play_var_separation', 'player_name',\n",
      "       'player_role', 'player_position', 'route_of_targeted_receiver',\n",
      "       'nums_frame_pre_throw', 'time_to_throw', 'tr_min_separation_first',\n",
      "       'tr_min_separation_last', 'tr_last_dir'],\n",
      "      dtype='object')\n",
      "Index(['game_id', 'play_id', 'nfl_id', 'qb_play_min_separation',\n",
      "       'qb_play_avg_separation', 'qb_play_var_separation',\n",
      "       'max_pressure_velocity', 'pressure_velocity_avg', 'pressure_volatility',\n",
      "       'max_pressure_acceleration', 'pressure_acceleration_avg',\n",
      "       'max_converging_defenders', 'avg_converging_defenders',\n",
      "       'std_converging_defenders', 'max_defenders_immediate_zone',\n",
      "       'max_defenders_closing_zone', 'avg_defenders_closing_zone',\n",
      "       'max_defenders_potential_zone', 'player_name', 'player_role'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Aggregate TR separately\n",
    "tr_play_features = tr_calculate_play_level_aggregates(tr_separation_df)\n",
    "print(tr_play_features.columns)\n",
    "print(complete_qb_play_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6661b",
   "metadata": {},
   "source": [
    "### Play Level - QB + TR + Supplementary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fb6a291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CORRECTED: TRACKING DATA ONLY - DROP INCOMPLETE PLAYS\n",
      "======================================================================\n",
      "Strategy: Inner joins - only keep plays with QB AND TR tracking data\n",
      "\n",
      "[STEP 1] QB pressure data as base:\n",
      "  • QB plays with pressure metrics: 14,105\n",
      "  • These have actual tracking-derived features\n",
      "\n",
      "[STEP 2] After QB + TR inner join:\n",
      "  • Plays with BOTH QB and TR data: 14,105\n",
      "  • QB-only plays dropped: 0\n",
      "  • Retention rate: 100.0%\n",
      "  • time_to_throw missing: 0 (should be 0)\n",
      "\n",
      "[STEP 3] After adding supplementary data (inner join):\n",
      "  • Final plays with complete data: 14,105\n",
      "  • Tracking-only plays dropped: 0\n",
      "  • Total supplementary plays: 18,009\n",
      "  • Supplementary plays used: 14,105 (78.3%)\n",
      "  • Plays with target variable: 14,105\n",
      "  ✓ All plays have target variable (perfect)\n",
      "\n",
      "[SUMMARY] Data Quality:\n",
      "  ✓ All plays have QB pressure metrics (tracking-derived)\n",
      "  ✓ All plays have TR coverage metrics (tracking-derived)\n",
      "  ✓ All plays have game context (supplementary)\n",
      "  ✓ No missing time_to_throw (from TR tracking)\n",
      "  ✓ High-quality dataset: 14,105 complete plays\n",
      "======================================================================\n",
      "\n",
      "Missing Values Summary:\n",
      "route_of_targeted_receiver     2\n",
      "pass_location_type            15\n",
      "team_coverage_man_zone         2\n",
      "team_coverage_type             2\n",
      "dtype: int64\n",
      "\n",
      "VERIFICATION:\n",
      "Original TR time_to_throw missing: 0\n",
      "Final corrected time_to_throw missing: 0\n",
      "Final dataset shape: (14105, 40)\n"
     ]
    }
   ],
   "source": [
    "# Merge play-level (clean, complete)\n",
    "final_ml_features = merge_play_level_features(\n",
    "    complete_qb_play_features,  # ← Complete QB features (all in one)\n",
    "    tr_play_features,           # ← TR features\n",
    "    supplementary_data          # ← Context + target\n",
    ")\n",
    "\n",
    "# Missing values summary\n",
    "missing_summary = final_ml_features.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(missing_summary)\n",
    "\n",
    "# save final dataset with QB Pressure index as csv\n",
    "final_ml_features.to_csv('data/output/final_ml_features_with_qb_pressure.csv', index=False)\n",
    "\n",
    "# Verify no missing time_to_throw\n",
    "print(\"\\nVERIFICATION:\")\n",
    "print(f\"Original TR time_to_throw missing: {tr_play_features['time_to_throw'].isna().sum()}\")\n",
    "print(f\"Final corrected time_to_throw missing: {final_ml_features['time_to_throw'].isna().sum()}\")\n",
    "print(f\"Final dataset shape: {final_ml_features.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e921dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Flow Tracking:\n",
      "1. Supplementary data: 18,009 plays\n",
      "2. QB frame-level: 396,765 QB-frames\n",
      "3. QB play-level: 14,105 plays\n",
      "4. TR play-level: 14,107 plays\n",
      "5. Final ML features: 14,105 plays\n",
      "\n",
      "Week distribution in supplementary:\n",
      "week\n",
      "1      819\n",
      "2      850\n",
      "3      904\n",
      "4      779\n",
      "5      742\n",
      "6      793\n",
      "7      693\n",
      "8      827\n",
      "9      711\n",
      "10     721\n",
      "11     707\n",
      "12     854\n",
      "13     666\n",
      "14    1460\n",
      "15    1594\n",
      "16    1686\n",
      "17    1603\n",
      "18    1600\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Flow Tracking:\")\n",
    "print(f\"1. Supplementary data: {len(supplementary_data):,} plays\")\n",
    "print(f\"2. QB frame-level: {len(qb_separation_frame_level):,} QB-frames\")\n",
    "print(f\"3. QB play-level: {len(complete_qb_play_features):,} plays\")  \n",
    "print(f\"4. TR play-level: {len(tr_play_features):,} plays\")\n",
    "print(f\"5. Final ML features: {len(final_ml_features):,} plays\")\n",
    "\n",
    "# Check week distribution\n",
    "if 'week' in supplementary_data.columns:\n",
    "    week_counts = supplementary_data['week'].value_counts().sort_index()\n",
    "    print(f\"\\nWeek distribution in supplementary:\")\n",
    "    print(week_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1a723",
   "metadata": {},
   "source": [
    "### Pressure components score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9539c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING PRESSURE COMPONENT SCORES\n",
      "======================================================================\n",
      "Components:\n",
      "  1. Convergence Pressure: Defender convergence\n",
      "  2. Velocity Pressure: Velocity pressure\n",
      "  3. TR Coverage Pressure: TR separation\n",
      "----------------------------------------------------------------------\n",
      "[CONVERGENCE PRESSURE]\n",
      "  • Source: max_converging_defenders\n",
      "  • Raw range: 0.0 - 5.0\n",
      "  • 95th percentile: 3.0\n",
      "  • Score range: 0.0 - 100.0\n",
      "  • Score mean: 13.4\n",
      "\n",
      "[VELOCITY PRESSURE]\n",
      "  • Source: max_pressure_velocity\n",
      "  • Raw range: -10.4 - 2.4 yds/sec\n",
      "  • Clipped range: -10.4 - 0.0\n",
      "  • Score range: 0.0 - 69.3\n",
      "  • Score mean: 5.6\n",
      "\n",
      "[COVERAGE PRESSURE]\n",
      "  • Source: tr_play_min_separation\n",
      "  • Raw range: 0.0 - 19.8 yards\n",
      "  • Score range: 0.0 - 100.0\n",
      "  • Score mean: 67.5\n",
      "  • Logic: Lower separation = Higher pressure\n",
      "\n",
      "==================================================\n",
      "COMPONENT SCORES SUMMARY\n",
      "==================================================\n",
      "Component Statistics:\n",
      "  • Spatial (Convergence):  13.4 ± 25.3 (range:  0.0-100.0)\n",
      "  • Temporal (Velocity) :   5.6 ±  9.2 (range:  0.0-69.3)\n",
      "  • Coverage (TR Sep)   :  67.5 ± 25.3 (range:  0.0-100.0)\n",
      "\n",
      "Component Correlations:\n",
      "  • Spatial (Convergence) ↔ Temporal (Velocity) : +0.109\n",
      "  • Spatial (Convergence) ↔ Coverage (TR Sep)   : +0.052\n",
      "  • Temporal (Velocity)  ↔ Coverage (TR Sep)   : +0.140\n",
      "\n",
      "✓ All components successfully calculated\n",
      "==================================================\n",
      "\n",
      "✓ Pressure component scores added to dataset\n",
      "✓ New columns: spatial_pressure_score, temporal_pressure_score, coverage_pressure_score\n",
      "✓ Dataset shape: (14105, 43)\n",
      "\n",
      "Sample Results (First 10 plays):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "game_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "play_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "convergence_pressure_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "velocity_pressure_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tr_coverage_pressure_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pass_result",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2d673044-8a1e-4dbf-8df5-ba16584cc529",
       "rows": [
        [
         "0",
         "2023090700",
         "101",
         "0.0",
         "0.0",
         "89.3",
         "I"
        ],
        [
         "1",
         "2023090700",
         "194",
         "33.3",
         "10.7",
         "90.0",
         "C"
        ],
        [
         "2",
         "2023090700",
         "219",
         "0.0",
         "0.0",
         "50.9",
         "C"
        ],
        [
         "3",
         "2023090700",
         "361",
         "33.3",
         "8.7",
         "78.6",
         "C"
        ],
        [
         "4",
         "2023090700",
         "436",
         "0.0",
         "10.7",
         "71.3",
         "C"
        ],
        [
         "5",
         "2023090700",
         "461",
         "100.0",
         "0.0",
         "76.7",
         "C"
        ],
        [
         "6",
         "2023090700",
         "530",
         "0.0",
         "0.7",
         "65.6",
         "C"
        ],
        [
         "7",
         "2023090700",
         "621",
         "0.0",
         "14.7",
         "85.3",
         "C"
        ],
        [
         "8",
         "2023090700",
         "713",
         "0.0",
         "0.0",
         "86.1",
         "I"
        ],
        [
         "9",
         "2023090700",
         "736",
         "0.0",
         "0.0",
         "79.2",
         "C"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>play_id</th>\n",
       "      <th>convergence_pressure_score</th>\n",
       "      <th>velocity_pressure_score</th>\n",
       "      <th>tr_coverage_pressure_score</th>\n",
       "      <th>pass_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.3</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>194</td>\n",
       "      <td>33.3</td>\n",
       "      <td>10.7</td>\n",
       "      <td>90.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>361</td>\n",
       "      <td>33.3</td>\n",
       "      <td>8.7</td>\n",
       "      <td>78.6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>71.3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>461</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.7</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>65.6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>85.3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.1</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023090700</td>\n",
       "      <td>736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      game_id  play_id  convergence_pressure_score  velocity_pressure_score  \\\n",
       "0  2023090700      101                         0.0                      0.0   \n",
       "1  2023090700      194                        33.3                     10.7   \n",
       "2  2023090700      219                         0.0                      0.0   \n",
       "3  2023090700      361                        33.3                      8.7   \n",
       "4  2023090700      436                         0.0                     10.7   \n",
       "5  2023090700      461                       100.0                      0.0   \n",
       "6  2023090700      530                         0.0                      0.7   \n",
       "7  2023090700      621                         0.0                     14.7   \n",
       "8  2023090700      713                         0.0                      0.0   \n",
       "9  2023090700      736                         0.0                      0.0   \n",
       "\n",
       "   tr_coverage_pressure_score pass_result  \n",
       "0                        89.3           I  \n",
       "1                        90.0           C  \n",
       "2                        50.9           C  \n",
       "3                        78.6           C  \n",
       "4                        71.3           C  \n",
       "5                        76.7           C  \n",
       "6                        65.6           C  \n",
       "7                        85.3           C  \n",
       "8                        86.1           I  \n",
       "9                        79.2           C  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (14105, 43)\n",
      "  Contains: All features for ML pipeline\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE THE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "# Create the pressure component scores\n",
    "if 'final_ml_features' in locals():\n",
    "    final_ml_features_with_scores = create_pressure_component_scores(\n",
    "        final_ml_features,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Pressure component scores added to dataset\")\n",
    "    print(f\"✓ New columns: spatial_pressure_score, temporal_pressure_score, coverage_pressure_score\")\n",
    "    print(f\"✓ Dataset shape: {final_ml_features_with_scores.shape}\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(f\"\\nSample Results (First 10 plays):\")\n",
    "    sample_cols = [\n",
    "        'game_id', 'play_id',\n",
    "        'convergence_pressure_score', 'velocity_pressure_score', 'tr_coverage_pressure_score',\n",
    "        'pass_result'\n",
    "    ]\n",
    "    available_sample_cols = [col for col in sample_cols if col in final_ml_features_with_scores.columns]\n",
    "    \n",
    "    if available_sample_cols:\n",
    "        display(final_ml_features_with_scores[available_sample_cols].head(10))\n",
    "    \n",
    "    # Save the enhanced dataset\n",
    "    output_path = 'data/output/final_ml_features_with_component_scores.csv'\n",
    "    final_ml_features_with_scores.to_csv(output_path, index=False)\n",
    "    print(f\"  Shape: {final_ml_features_with_scores.shape}\")\n",
    "    print(f\"  Contains: All features for ML pipeline\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ final_ml_features not found. Please run the previous pipeline steps first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "835ca1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Component Analysis:\n",
      "  • convergence_pressure_score: 13.4 avg\n",
      "  • velocity_pressure_score  : 5.6 avg\n",
      "  • tr_coverage_pressure_score: 67.5 avg\n",
      "\n",
      "Component Correlations:\n",
      "                            convergence_pressure_score  \\\n",
      "convergence_pressure_score                        1.00   \n",
      "velocity_pressure_score                           0.11   \n",
      "tr_coverage_pressure_score                        0.05   \n",
      "\n",
      "                            velocity_pressure_score  \\\n",
      "convergence_pressure_score                     0.11   \n",
      "velocity_pressure_score                        1.00   \n",
      "tr_coverage_pressure_score                     0.14   \n",
      "\n",
      "                            tr_coverage_pressure_score  \n",
      "convergence_pressure_score                        0.05  \n",
      "velocity_pressure_score                           0.14  \n",
      "tr_coverage_pressure_score                        1.00  \n",
      "\n",
      "✓ No excessive correlations found\n"
     ]
    }
   ],
   "source": [
    "# Analyze component relationships\n",
    "pressure_components = [\n",
    "    'convergence_pressure_score',\n",
    "    'velocity_pressure_score', \n",
    "    'tr_coverage_pressure_score'\n",
    "]\n",
    "\n",
    "print(\"\\nComponent Analysis:\")\n",
    "for component in pressure_components:\n",
    "    if component in final_ml_features_with_scores.columns:\n",
    "        mean_val = final_ml_features_with_scores[component].mean()\n",
    "        print(f\"  • {component:25s}: {mean_val:.1f} avg\")\n",
    "\n",
    "# Check for excessive correlation\n",
    "if all(col in final_ml_features_with_scores.columns for col in pressure_components):\n",
    "    correlation_matrix = final_ml_features_with_scores[pressure_components].corr()\n",
    "    print(f\"\\nComponent Correlations:\")\n",
    "    print(correlation_matrix.round(2))\n",
    "    \n",
    "    # Flag high correlations (> 0.7)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(pressure_components)):\n",
    "        for j in range(i+1, len(pressure_components)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr) > 0.7:\n",
    "                high_corr_pairs.append((pressure_components[i], pressure_components[j], corr))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nHigh Correlation Warnings (>0.7):\")\n",
    "        for comp1, comp2, corr in high_corr_pairs:\n",
    "            print(f\"  ⚠ {comp1} ↔ {comp2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n✓ No excessive correlations found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35d9b0",
   "metadata": {},
   "source": [
    "### Validate and Display sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06864680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Validating final features for ML pipeline...\n",
      "\n",
      "======================================================================\n",
      "VALIDATING FINAL FEATURES FOR ML PIPELINE\n",
      "======================================================================\n",
      "Target column: 'pass_result'\n",
      "----------------------------------------------------------------------\n",
      "✓ CHECK 1 PASSED: Target variable 'pass_result' available\n",
      "    Missing: 0 (0.0%)\n",
      "✓ CHECK 2 PASSED: Sufficient samples (14,105)\n",
      "    → Excellent sample size for Random Forest\n",
      "✗ CHECK 3 FAILED: Expected 2 classes, found 3\n",
      "    Classes: ['I' 'C' 'IN']\n",
      "⊘ CHECK 4 SKIPPED: No valid target variable\n",
      "✓ CHECK 5 PASSED: No duplicate play records\n",
      "✓ CHECK 6 PASSED: Numeric features valid (32 features)\n",
      "\n",
      "======================================================================\n",
      "⚠⚠⚠ DATASET NEEDS ATTENTION BEFORE ML ⚠⚠⚠\n",
      "======================================================================\n",
      "\n",
      "Failed Checks (must be resolved):\n",
      "  ✗ target_is_binary\n",
      "\n",
      "Action Required:\n",
      "  1. Address failed checks above\n",
      "  2. Re-run validation\n",
      "  3. Proceed only after all checks pass\n",
      "======================================================================\n",
      "\n",
      "⚠ Validation complete - REVIEW WARNINGS ABOVE ⚠\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Validate Final Features for ML Readiness\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n[4/4] Validating final features for ML pipeline...\")\n",
    "\n",
    "try:\n",
    "    validation_results = validate_final_features(\n",
    "        final_ml_features_with_scores,\n",
    "        target_column='pass_result'\n",
    "    )\n",
    "    \n",
    "    if validation_results['ready_for_ml']:\n",
    "        print(f\"\\n✓ Validation complete - READY FOR ML PIPELINE ✓\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Validation complete - REVIEW WARNINGS ABOVE ⚠\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error during validation: {e}\")\n",
    "    print(\"⚠ Proceeding without full validation (not critical)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58b2c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAMPLE: FINAL ML FEATURES\n",
      "======================================================================\n",
      "   game_id  play_id  qb_play_avg_separation  max_converging_defenders  tr_play_avg_separation  time_to_throw pass_result\n",
      "2023090700      101               10.694615                         0                    2.77            2.6           I\n",
      "2023090700      194                7.291563                         1                    4.19            3.2           C\n",
      "2023090700      219               10.027059                         0                    5.93            1.7           C\n",
      "2023090700      361               13.231961                         1                    6.06            5.1           C\n",
      "2023090700      436                8.061500                         0                    4.65            2.0           C\n",
      "2023090700      461                7.239130                         3                    3.18            2.3           C\n",
      "2023090700      530               10.251500                         0                    5.07            2.0           C\n",
      "2023090700      621               10.095517                         0                    2.99            2.9           C\n",
      "2023090700      713               11.614167                         0                    4.96            2.4           I\n",
      "2023090700      736               10.405806                         0                    2.71            3.1           C\n",
      "\n",
      "======================================================================\n",
      "✓ STEP A.3 COMPLETE - CONVERGENCE INTEGRATION FINISHED\n",
      "======================================================================\n",
      "\n",
      "🎉 DEFENDER CONVERGENCE FEATURE ENGINEERING COMPLETE! 🎉\n",
      "\n",
      "All Steps Summary:\n",
      "  ✓ A.1 - Data Validation (defender positions confirmed)\n",
      "  ✓ A.2 - Core Convergence Calculation (frame + play level)\n",
      "  ✓ A.3 - Integration with Pocket Collapse (enhanced detection)\n",
      "\n",
      "Dataset Statistics:\n",
      "  • Frame-level records: 396,765\n",
      "  • Play-level records: 14,105\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# DISPLAY SAMPLE RESULTS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE: FINAL ML FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_cols = [\n",
    "    'game_id', 'play_id', 'player_name_qb',\n",
    "    'qb_play_avg_separation', 'max_converging_defenders',\n",
    "    'tr_play_avg_separation', 'time_to_throw',\n",
    "    'pass_result'\n",
    "]\n",
    "\n",
    "available_sample_cols = [col for col in sample_cols if col in final_ml_features_with_scores.columns]\n",
    "\n",
    "if available_sample_cols:\n",
    "    print(final_ml_features[available_sample_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"⚠ Sample columns not available, showing first 5 columns:\")\n",
    "    print(final_ml_features_with_scores.iloc[:, :5].head(10))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMPLETION SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ STEP A.3 COMPLETE - CONVERGENCE INTEGRATION FINISHED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n🎉 DEFENDER CONVERGENCE FEATURE ENGINEERING COMPLETE! 🎉\")\n",
    "print(\"\\nAll Steps Summary:\")\n",
    "print(\"  ✓ A.1 - Data Validation (defender positions confirmed)\")\n",
    "print(\"  ✓ A.2 - Core Convergence Calculation (frame + play level)\")\n",
    "print(\"  ✓ A.3 - Integration with Pocket Collapse (enhanced detection)\")\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"  • Frame-level records: {qb_features_frame_level.shape[0]:,}\")\n",
    "print(f\"  • Play-level records: {final_ml_features_with_scores.shape[0]:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
